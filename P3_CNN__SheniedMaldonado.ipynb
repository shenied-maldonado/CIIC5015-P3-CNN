{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4NMugQEb18Q"
      },
      "source": [
        "# CIIC5015 - Introduction to Artificial Intelligence\n",
        "Project III - Convolutional Neural Networks\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Objectives\n",
        "1. Understand the design, implementation and use of deep learning models.\n",
        "2. Understand the use of the convolutional neural networks for classification.\n",
        "3. Gain experience by implementing AI applications using modern deep learning\n",
        "frameworks.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Overview\n",
        "You will design, implement and test a neural network model using PyTorch and Jupyter\n",
        "notebooks. These networks will be used to implement classification tasks on an image dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZsokgWBdbQ2"
      },
      "source": [
        "## Classification\n",
        "\n",
        "Classification\n",
        "You will use the MINIST to develop a model that predicts class for each digit. Use the code\n",
        "from p2 as a template, but now used convolutional layer. Here is a example on how to build a\n",
        "convolutional network in PyTorch.\n",
        "\n",
        "https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
        "\n",
        "You can follow the same steps as in p2. But you will use the following networks:\n",
        "\n",
        "1. Network 1 : 4-layer network\n",
        " * Layer 1 – convolution with 16 filters, each filter 5x5 with same padding\n",
        " * Layer 2 – ReLU activation\n",
        " * Layer 3 – Flatten layer\n",
        " * Layer 4 – fully connected layer with 10 neurons as output (using cross entropy\n",
        "loss does the softmax)\n",
        "\n",
        "2. Network 2 : 8-layer network\n",
        " * Layer 1 – convolution with 6 filters, each filter 5x5 with same padding\n",
        " * Layer 2 – ReLU activation\n",
        " * Layer 3 – convolution with 16 filters , each filter 5x5 with same padding\n",
        " * Layer 4 – ReLU activation\n",
        " * Layer 5 – flatten layer\n",
        " * Leyer 6 – fully connected layer with 84 neurons as ouput\n",
        " * Layer 7 – ReLU activation\n",
        " * Layer 8 - fully connected layer with 10 neurons as output (using cross entropy\n",
        "loss does the softmax)\n",
        "\n",
        "3. Network 3 : 14-layer network\n",
        " * Layer 1 – convolution with 6 filters , each filter 5x5 with same padding\n",
        " * Layer 2 – batch normalization for 6 filters\n",
        " * Layer 3 – ReLU activation\n",
        " * Layer 4 – Max pooling of size 2 (to halve de image)\n",
        " * Layer 5 – convolution with 16 filters , each filter 5x5 with same padding\n",
        " * Layer 6 – batch normalization for 16 filters\n",
        " * Layer 7 – ReLU activation\n",
        " * Layer 8 – Max pooling of size 2 (to halve de image)\n",
        " * Layer 9 – flatten layer\n",
        " * Leyer 10 – fully connected layer with 120 neurons as ouput\n",
        " * Layer 11 – ReLU activation\n",
        " * Leyer 12 – fully connected layer with 84 neurons as ouput\n",
        " * Layer 13 – ReLU activation\n",
        " * Layer 14 - fully connected layer with 10 neurons as output (using cross entropy\n",
        "loss does the softmax)\n",
        "\n",
        "All networks will use ReLU activation in the intermediate layers, but not in the final. Train for\n",
        "20 epochs with learning rate of 0.01."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjSoIrPsfrOu"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "c0NcYccnc79D"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "from torchsummary import summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOiN3nQ5irvu"
      },
      "source": [
        "### Prepare the data\n",
        "The first step is to collect and preprocess the data. This includes separating the data into training and testing sets as well as creating their respective loaders. \n",
        "\n",
        "\n",
        "```\n",
        "### Class Distribution\n",
        "--> There are a total of 70000 instances.\n",
        "\n",
        "LABELS        ID\n",
        "zero          0\n",
        "one           1\n",
        "two           2\n",
        "three         3\n",
        "four          4\n",
        "five          5\n",
        "six           6\n",
        "seven         7\n",
        "eight         8\n",
        "nine          9\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU-jbcfndqKZ"
      },
      "source": [
        "Load and normalize training and test datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4TT2sWBr_Dn8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a093191f-1cc8-4247-ecb7-c11dd8111416"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 45370994.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../../data/MNIST/raw/train-images-idx3-ubyte.gz to ../../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 66962793.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1648877/1648877 [00:00<00:00, 27049223.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 12734310.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../../data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.Grayscale(),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize(mean=0.5, std=0.5)])\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "# Load MNIST datasets\n",
        "train_dataset = torchvision.datasets.MNIST(root='../../data', \n",
        "                                           train=True, \n",
        "                                           transform=transform,  \n",
        "                                           download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='../../data', \n",
        "                                          train=False, \n",
        "                                          transform=transform)\n",
        "\n",
        "# Split training set into training and validation set\n",
        "trainset_size = len(train_dataset)\n",
        "validset_size = int(0.2 * trainset_size)  # validation set --> 20%\n",
        "trainset_size -= validset_size  # training set --> 80%\n",
        "train_dataset, valid_dataset = torch.utils.data.random_split(train_dataset,\n",
        "                                                             [trainset_size, validset_size])\n",
        "\n",
        "# Data loaders\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset, \n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=False)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n",
        "\n",
        "# Classes information\n",
        "classes = {\n",
        " 0: 'zero',\n",
        " 1: 'one',\n",
        " 2: 'two',\n",
        " 3: 'three',\n",
        " 4: 'four',\n",
        " 5: 'five',\n",
        " 6: 'six',\n",
        " 7: 'seven',\n",
        " 8: 'eight',\n",
        " 9: 'nine'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoIrQ1uOd0Tl"
      },
      "source": [
        "Inspect the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFnGEEp_op4w",
        "outputId": "295eb413-9640-4216-9137-5475ae1aa5b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " <torch.utils.data.dataset.Subset object at 0x7f383d475030>\n",
            "\n",
            " Dataset MNIST\n",
            "    Number of datapoints: 10000\n",
            "    Root location: ../../data\n",
            "    Split: Test\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               Grayscale(num_output_channels=1)\n",
            "               ToTensor()\n",
            "               Normalize(mean=0.5, std=0.5)\n",
            "           )\n",
            "\n",
            " <torch.utils.data.dataset.Subset object at 0x7f383d475210>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "58000"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "print('\\n', train_dataset)\n",
        "print('\\n', test_dataset)\n",
        "print('\\n', valid_dataset)\n",
        "\n",
        "len(train_dataset + test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "BMWYZaF_Ap7X",
        "outputId": "db177d09-efca-49b6-9016-8fee78b9ac5e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "six   three five  five \n"
          ]
        }
      ],
      "source": [
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "\n",
        "# print labels\n",
        "print(' '.join(f'{classes[labels.numpy()[j]]:5s}' for j in range(batch_size)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDBNgf4Ajx8q"
      },
      "source": [
        "### Define the model architecture\n",
        "Choose the number and categorization of your layers for your models. Then, define the loss function and optimizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZdDKfThs9sJ"
      },
      "source": [
        "CNN1 -> 4-layer network\n",
        " * Layer 1 – convolution with 16 filters, each filter 5x5 with same padding\n",
        " * Layer 2 – ReLU activation\n",
        " * Layer 3 – Flatten layer\n",
        " * Layer 4 – fully connected layer with 10 neurons as output (using cross entropy\n",
        "loss does the softmax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zqjqiENUsUQa"
      },
      "outputs": [],
      "source": [
        "class CNN1(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN1, self).__init__()\n",
        "    self.conv = nn.Conv2d(in_channels=1, out_channels=16,\n",
        "                          kernel_size=5, padding=2)\n",
        "    self.reLU = nn.ReLU()\n",
        "    self.fc = nn.Linear(in_features=16*28*28, out_features=10)\n",
        "    self.soft = nn.Softmax(dim=1)\n",
        "\n",
        "\n",
        "  def forward(self, x):     \n",
        "    x = self.reLU(self.conv(x))        # convolution layer - 16 5x5 filters\n",
        "    x = torch.flatten(x,1)             # flatten all dimensions except batch                    # fully connected layer - 10 neurons as output\n",
        "    x = self.soft(self.fc(x))          # fully connected layer - 10 output neurons\n",
        "\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMgMUfMFgsjm"
      },
      "source": [
        "CNN2 -> 8-layer network\n",
        "* Layer 1 - convolution with 6 filters, each filter 5x5 with same padding\n",
        "* Layer 2 - ReLU activation\n",
        "* Layer 3 - convolution with 16 filters, each filer 5x5 with same padding\n",
        "* Layer 4 - ReLU activation\n",
        "* Layer 5 - flatten layer\n",
        "* Layer 6 - fully connected layer with 84 neurons as output\n",
        "* Layer 7 - ReLU activation\n",
        "* Layer 8 - fully connected layer with 10 neurons as output (using cross entropy\n",
        "loss does the softmax)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jQMgrveEiFVL"
      },
      "outputs": [],
      "source": [
        "class CNN2(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN2, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, \n",
        "                           kernel_size=5, padding=2)\n",
        "    self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, \n",
        "                           kernel_size=5, padding=5)\n",
        "    self.reLU = nn.ReLU()\n",
        "    self.fc1 = nn.Linear(in_features=16*34*34, out_features=84)\n",
        "    self.fc2 = nn.Linear(in_features=84, out_features=10)\n",
        "    self.soft = nn.Softmax(dim=1)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.reLU(self.conv1(x))    # convolutional layer - 6 5x5 filters\n",
        "    x = self.reLU(self.conv2(x))    # convolutional layer - 16 5x5 filters\n",
        "    x = torch.flatten(x, 1)         # flatten all dimensions except batch\n",
        "    x = self.reLU(self.fc1(x))      # fully connected layer - 84 output neurons\n",
        "    x = self.soft(self.fc2(x))      # fully connected layer - 10 output neurons\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jya0uRa1naCs"
      },
      "source": [
        "CNN3 -> 14-layer network\n",
        "* Layer 1 - convolution with 6 filters, each filter 5x5 with same padding\n",
        "* Layer 2 - batch normalization\n",
        "* Layer 3 - ReLu activation\n",
        "* Layer 4 - max pooling of size 2\n",
        "* Layer 5 - convolution with 16 filters, each filter 5x5 with same padding\n",
        "* Layer 6 - batch normalization for 16 filters\n",
        "* Layer 7 - ReLU activation\n",
        "* Layer 8 - max pooling of size 2\n",
        "* Layer 9 - flatten layer\n",
        "* Layer 10 - fully connected layer with 120 neurons as output\n",
        "* Layer 11 - ReLU activation\n",
        "* Layer 12 - fully connected layer with 84 neurons as output\n",
        "* Layer 13 - ReLU activation\n",
        "* Layer 14 - fully connected layer with 10 neurons as output (using cross entropy loss does the softmax)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "cKa8O41NoqRc"
      },
      "outputs": [],
      "source": [
        "class CNN3(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN3, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, \n",
        "                           kernel_size=5, padding=2)\n",
        "    self.conv2 = nn.Conv2d(in_channels=6, out_channels=16,\n",
        "                           kernel_size=5, padding=2)\n",
        "    self.norm1 = nn.BatchNorm2d(6)\n",
        "    self.norm2 = nn.BatchNorm2d(16)\n",
        "    self.reLU = nn.ReLU()\n",
        "    self.maxp = nn.MaxPool2d(kernel_size=2)\n",
        "    self.fc1 = nn.Linear(in_features=16*7*7, out_features= 120)\n",
        "    self.fc2 = nn.Linear(in_features=120, out_features=84)\n",
        "    self.fc3 = nn.Linear(in_features=84, out_features=10)\n",
        "    self.soft = nn.Softmax(dim=1)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.maxp(self.reLU(self.norm1(self.conv1(x))))\n",
        "        # convolutional layer - 6 5x5 filters\n",
        "        # batch normalization - 6 filters\n",
        "        # reLU activation\n",
        "        # max pooling\n",
        "        \n",
        "    x = self.maxp(self.reLU(self.norm2(self.conv2(x))))\n",
        "        # convolutional layer - 16 5x5 filters\n",
        "        # batch normalization - 16 filters\n",
        "        # reLU activation\n",
        "        # max pooling\n",
        "\n",
        "    x = torch.flatten(x,1)\n",
        "        # flatten all dimensions expect batch\n",
        "\n",
        "    x = self.reLU(self.fc1(x))    \n",
        "        # fully connected layer - 120 output neurons\n",
        "        # reLU activation    \n",
        "\n",
        "    x = self.reLU(self.fc2(x))\n",
        "        # fully connected layer - 84 output neurons\n",
        "        # reLU activation  \n",
        "\n",
        "    x = self.soft(self.fc3(x))\n",
        "        # fully connecter layer - 10 output neurons\n",
        "        # softmax\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_tqmJzWUlUV"
      },
      "source": [
        "### Train the model\n",
        "Training a model is a critical step in the machine learning pipeline as it involves the process of optimizing the model to perform well on a given task. The goal of training a model is to minimize the difference between its predicted outputs and the actual outputs (i.e., labels) in the training dataset. \n",
        "\n",
        "Assuming the data has already been pre-processed, training a model involves several tasks:\n",
        "\n",
        "1. Model initialization: defining the structure of the neural network and initializing the weights and biases.\n",
        "\n",
        "2. Forward pass: passing input data through the neural network to generate output predictions.\n",
        "\n",
        "3. Compute loss: comparing the output predictions to the ground truth labels to calculate the loss or error.\n",
        "\n",
        "5. Backward pass: propagating the loss backwards through the neural network to calculate the gradients of the weights and biases with respect to the loss.\n",
        "\n",
        "7. Parameter updates: adjusting the weights and biases of the neural network based on the gradients calculated in the backward pass using optimization algorithms such as stochastic gradient descent (SGD).\n",
        "\n",
        "8. Iteration: repeating the forward pass, backward pass, and parameter updates for multiple epochs until the model converges and the loss is minimized.\n",
        "\n",
        "The goal of training a model is to minimize the loss on the training data while also achieving good performance on new, unseen data. This process involves finding the optimal balance between overfitting and underfitting, where the model is able to generalize well to new data without memorizing the training data.\n",
        "\n",
        "The importance of training a model lies in its ability to generalize to new, unseen data. By minimizing the loss on the training data, the model can learn to recognize patterns in the data that generalize to new, unseen data. Without training, a model would not be able to learn from the data and would perform poorly on new, unseen data.\n",
        "\n",
        "In summary, training a model is crucial for optimizing its parameters to minimize the difference between its predictions and actual outputs, thereby improving its ability to generalize to new, unseen data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVEa0eDJBeu2"
      },
      "source": [
        "Prepare to train."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trcmmQBj1vvd",
        "outputId": "809f8650-9dd7-4a26-ba06-d93e1d106c92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN1: 4-layer \n",
            "\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 28, 28]             416\n",
            "              ReLU-2           [-1, 16, 28, 28]               0\n",
            "            Linear-3                   [-1, 10]         125,450\n",
            "           Softmax-4                   [-1, 10]               0\n",
            "================================================================\n",
            "Total params: 125,866\n",
            "Trainable params: 125,866\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.19\n",
            "Params size (MB): 0.48\n",
            "Estimated Total Size (MB): 0.67\n",
            "----------------------------------------------------------------\n",
            "CNN2: 8-layer \n",
            "\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [-1, 6, 28, 28]             156\n",
            "              ReLU-2            [-1, 6, 28, 28]               0\n",
            "            Conv2d-3           [-1, 16, 34, 34]           2,416\n",
            "              ReLU-4           [-1, 16, 34, 34]               0\n",
            "            Linear-5                   [-1, 84]       1,553,748\n",
            "              ReLU-6                   [-1, 84]               0\n",
            "            Linear-7                   [-1, 10]             850\n",
            "           Softmax-8                   [-1, 10]               0\n",
            "================================================================\n",
            "Total params: 1,557,170\n",
            "Trainable params: 1,557,170\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.36\n",
            "Params size (MB): 5.94\n",
            "Estimated Total Size (MB): 6.30\n",
            "----------------------------------------------------------------\n",
            "CNN3: 14-layer \n",
            "\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [-1, 6, 28, 28]             156\n",
            "       BatchNorm2d-2            [-1, 6, 28, 28]              12\n",
            "              ReLU-3            [-1, 6, 28, 28]               0\n",
            "         MaxPool2d-4            [-1, 6, 14, 14]               0\n",
            "            Conv2d-5           [-1, 16, 14, 14]           2,416\n",
            "       BatchNorm2d-6           [-1, 16, 14, 14]              32\n",
            "              ReLU-7           [-1, 16, 14, 14]               0\n",
            "         MaxPool2d-8             [-1, 16, 7, 7]               0\n",
            "            Linear-9                  [-1, 120]          94,200\n",
            "             ReLU-10                  [-1, 120]               0\n",
            "           Linear-11                   [-1, 84]          10,164\n",
            "             ReLU-12                   [-1, 84]               0\n",
            "           Linear-13                   [-1, 10]             850\n",
            "          Softmax-14                   [-1, 10]               0\n",
            "================================================================\n",
            "Total params: 107,830\n",
            "Trainable params: 107,830\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.20\n",
            "Params size (MB): 0.41\n",
            "Estimated Total Size (MB): 0.61\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Check Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Set parameters\n",
        "num_epochs = 20\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Track errors\n",
        "train_losses = [[],[],[]]\n",
        "val_losses = [[],[],[]]\n",
        "\n",
        "# Initialize models\n",
        "model1 = CNN1().to(device)\n",
        "print(\"CNN1: 4-layer \\n\")\n",
        "summary(model1, (1,28,28))\n",
        "\n",
        "print(\"CNN2: 8-layer \\n\")\n",
        "model2 = CNN2().to(device)\n",
        "summary(model2, (1,28,28))\n",
        "\n",
        "print(\"CNN3: 14-layer \\n\")\n",
        "model3 = CNN3().to(device)\n",
        "summary(model3, (1,28,28))\n",
        "\n",
        "# Define loss function and optimizers\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer1 = torch.optim.SGD(model1.parameters(), lr=learning_rate)\n",
        "optimizer2 = torch.optim.SGD(model2.parameters(), lr=learning_rate)\n",
        "optimizer3 = torch.optim.SGD(model3.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhWu7kIshoen"
      },
      "source": [
        "CNN1 - Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "k16Si49P072N",
        "outputId": "dcdfc6af-2e58-4483-dae3-7a157ef5eaa3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/20], Step [500/12000], Training Loss: 2.0177\n",
            "Epoch [1/20], Step [1000/12000], Training Loss: 1.7462\n",
            "Epoch [1/20], Step [1500/12000], Training Loss: 1.7002\n",
            "Epoch [1/20], Step [2000/12000], Training Loss: 1.6690\n",
            "Epoch [1/20], Step [2500/12000], Training Loss: 1.6469\n",
            "Epoch [1/20], Step [3000/12000], Training Loss: 1.6293\n",
            "Epoch [1/20], Step [3500/12000], Training Loss: 1.6496\n",
            "Epoch [1/20], Step [4000/12000], Training Loss: 1.6294\n",
            "Epoch [1/20], Step [4500/12000], Training Loss: 1.6352\n",
            "Epoch [1/20], Step [5000/12000], Training Loss: 1.6001\n",
            "Epoch [1/20], Step [5500/12000], Training Loss: 1.5574\n",
            "Epoch [1/20], Step [6000/12000], Training Loss: 1.5369\n",
            "Epoch [1/20], Step [6500/12000], Training Loss: 1.5469\n",
            "Epoch [1/20], Step [7000/12000], Training Loss: 1.5335\n",
            "Epoch [1/20], Step [7500/12000], Training Loss: 1.5370\n",
            "Epoch [1/20], Step [8000/12000], Training Loss: 1.5393\n",
            "Epoch [1/20], Step [8500/12000], Training Loss: 1.5275\n",
            "Epoch [1/20], Step [9000/12000], Training Loss: 1.5233\n",
            "Epoch [1/20], Step [9500/12000], Training Loss: 1.5297\n",
            "Epoch [1/20], Step [10000/12000], Training Loss: 1.5192\n",
            "Epoch [1/20], Step [10500/12000], Training Loss: 1.5208\n",
            "Epoch [1/20], Step [11000/12000], Training Loss: 1.5182\n",
            "Epoch [1/20], Step [11500/12000], Training Loss: 1.5209\n",
            "Epoch [1/20], Step [12000/12000], Training Loss: 1.5150\n",
            "Epoch [2/20], Step [500/12000], Training Loss: 1.5051\n",
            "Epoch [2/20], Step [1000/12000], Training Loss: 1.5087\n",
            "Epoch [2/20], Step [1500/12000], Training Loss: 1.5102\n",
            "Epoch [2/20], Step [2000/12000], Training Loss: 1.5110\n",
            "Epoch [2/20], Step [2500/12000], Training Loss: 1.5068\n",
            "Epoch [2/20], Step [3000/12000], Training Loss: 1.5100\n",
            "Epoch [2/20], Step [3500/12000], Training Loss: 1.5054\n",
            "Epoch [2/20], Step [4000/12000], Training Loss: 1.5126\n",
            "Epoch [2/20], Step [4500/12000], Training Loss: 1.5113\n",
            "Epoch [2/20], Step [5000/12000], Training Loss: 1.5046\n",
            "Epoch [2/20], Step [5500/12000], Training Loss: 1.5073\n",
            "Epoch [2/20], Step [6000/12000], Training Loss: 1.5060\n",
            "Epoch [2/20], Step [6500/12000], Training Loss: 1.5013\n",
            "Epoch [2/20], Step [7000/12000], Training Loss: 1.5035\n",
            "Epoch [2/20], Step [7500/12000], Training Loss: 1.5057\n",
            "Epoch [2/20], Step [8000/12000], Training Loss: 1.4990\n",
            "Epoch [2/20], Step [8500/12000], Training Loss: 1.5003\n",
            "Epoch [2/20], Step [9000/12000], Training Loss: 1.5005\n",
            "Epoch [2/20], Step [9500/12000], Training Loss: 1.4976\n",
            "Epoch [2/20], Step [10000/12000], Training Loss: 1.4990\n",
            "Epoch [2/20], Step [10500/12000], Training Loss: 1.4988\n",
            "Epoch [2/20], Step [11000/12000], Training Loss: 1.5044\n",
            "Epoch [2/20], Step [11500/12000], Training Loss: 1.5029\n",
            "Epoch [2/20], Step [12000/12000], Training Loss: 1.5000\n",
            "Epoch [3/20], Step [500/12000], Training Loss: 1.4961\n",
            "Epoch [3/20], Step [1000/12000], Training Loss: 1.4947\n",
            "Epoch [3/20], Step [1500/12000], Training Loss: 1.4870\n",
            "Epoch [3/20], Step [2000/12000], Training Loss: 1.4980\n",
            "Epoch [3/20], Step [2500/12000], Training Loss: 1.5023\n",
            "Epoch [3/20], Step [3000/12000], Training Loss: 1.4906\n",
            "Epoch [3/20], Step [3500/12000], Training Loss: 1.4902\n",
            "Epoch [3/20], Step [4000/12000], Training Loss: 1.4954\n",
            "Epoch [3/20], Step [4500/12000], Training Loss: 1.4913\n",
            "Epoch [3/20], Step [5000/12000], Training Loss: 1.4917\n",
            "Epoch [3/20], Step [5500/12000], Training Loss: 1.4968\n",
            "Epoch [3/20], Step [6000/12000], Training Loss: 1.4909\n",
            "Epoch [3/20], Step [6500/12000], Training Loss: 1.4977\n",
            "Epoch [3/20], Step [7000/12000], Training Loss: 1.4974\n",
            "Epoch [3/20], Step [7500/12000], Training Loss: 1.4950\n",
            "Epoch [3/20], Step [8000/12000], Training Loss: 1.4919\n",
            "Epoch [3/20], Step [8500/12000], Training Loss: 1.4932\n",
            "Epoch [3/20], Step [9000/12000], Training Loss: 1.4913\n",
            "Epoch [3/20], Step [9500/12000], Training Loss: 1.5012\n",
            "Epoch [3/20], Step [10000/12000], Training Loss: 1.4960\n",
            "Epoch [3/20], Step [10500/12000], Training Loss: 1.4980\n",
            "Epoch [3/20], Step [11000/12000], Training Loss: 1.4939\n",
            "Epoch [3/20], Step [11500/12000], Training Loss: 1.4968\n",
            "Epoch [3/20], Step [12000/12000], Training Loss: 1.4882\n",
            "Epoch [4/20], Step [500/12000], Training Loss: 1.4861\n",
            "Epoch [4/20], Step [1000/12000], Training Loss: 1.4905\n",
            "Epoch [4/20], Step [1500/12000], Training Loss: 1.4845\n",
            "Epoch [4/20], Step [2000/12000], Training Loss: 1.4934\n",
            "Epoch [4/20], Step [2500/12000], Training Loss: 1.4913\n",
            "Epoch [4/20], Step [3000/12000], Training Loss: 1.4912\n",
            "Epoch [4/20], Step [3500/12000], Training Loss: 1.4863\n",
            "Epoch [4/20], Step [4000/12000], Training Loss: 1.4927\n",
            "Epoch [4/20], Step [4500/12000], Training Loss: 1.4934\n",
            "Epoch [4/20], Step [5000/12000], Training Loss: 1.4883\n",
            "Epoch [4/20], Step [5500/12000], Training Loss: 1.4889\n",
            "Epoch [4/20], Step [6000/12000], Training Loss: 1.4875\n",
            "Epoch [4/20], Step [6500/12000], Training Loss: 1.4868\n",
            "Epoch [4/20], Step [7000/12000], Training Loss: 1.4885\n",
            "Epoch [4/20], Step [7500/12000], Training Loss: 1.4881\n",
            "Epoch [4/20], Step [8000/12000], Training Loss: 1.4914\n",
            "Epoch [4/20], Step [8500/12000], Training Loss: 1.4869\n",
            "Epoch [4/20], Step [9000/12000], Training Loss: 1.4922\n",
            "Epoch [4/20], Step [9500/12000], Training Loss: 1.4902\n",
            "Epoch [4/20], Step [10000/12000], Training Loss: 1.4892\n",
            "Epoch [4/20], Step [10500/12000], Training Loss: 1.4891\n",
            "Epoch [4/20], Step [11000/12000], Training Loss: 1.4881\n",
            "Epoch [4/20], Step [11500/12000], Training Loss: 1.4927\n",
            "Epoch [4/20], Step [12000/12000], Training Loss: 1.4886\n",
            "Epoch [5/20], Step [500/12000], Training Loss: 1.4866\n",
            "Epoch [5/20], Step [1000/12000], Training Loss: 1.4842\n",
            "Epoch [5/20], Step [1500/12000], Training Loss: 1.4885\n",
            "Epoch [5/20], Step [2000/12000], Training Loss: 1.4851\n",
            "Epoch [5/20], Step [2500/12000], Training Loss: 1.4844\n",
            "Epoch [5/20], Step [3000/12000], Training Loss: 1.4867\n",
            "Epoch [5/20], Step [3500/12000], Training Loss: 1.4857\n",
            "Epoch [5/20], Step [4000/12000], Training Loss: 1.4908\n",
            "Epoch [5/20], Step [4500/12000], Training Loss: 1.4866\n",
            "Epoch [5/20], Step [5000/12000], Training Loss: 1.4853\n",
            "Epoch [5/20], Step [5500/12000], Training Loss: 1.4779\n",
            "Epoch [5/20], Step [6000/12000], Training Loss: 1.4832\n",
            "Epoch [5/20], Step [6500/12000], Training Loss: 1.4851\n",
            "Epoch [5/20], Step [7000/12000], Training Loss: 1.4823\n",
            "Epoch [5/20], Step [7500/12000], Training Loss: 1.4816\n",
            "Epoch [5/20], Step [8000/12000], Training Loss: 1.4854\n",
            "Epoch [5/20], Step [8500/12000], Training Loss: 1.4836\n",
            "Epoch [5/20], Step [9000/12000], Training Loss: 1.4903\n",
            "Epoch [5/20], Step [9500/12000], Training Loss: 1.4842\n",
            "Epoch [5/20], Step [10000/12000], Training Loss: 1.4860\n",
            "Epoch [5/20], Step [10500/12000], Training Loss: 1.4823\n",
            "Epoch [5/20], Step [11000/12000], Training Loss: 1.4898\n",
            "Epoch [5/20], Step [11500/12000], Training Loss: 1.4874\n",
            "Epoch [5/20], Step [12000/12000], Training Loss: 1.4869\n",
            "Epoch [6/20], Step [500/12000], Training Loss: 1.4800\n",
            "Epoch [6/20], Step [1000/12000], Training Loss: 1.4787\n",
            "Epoch [6/20], Step [1500/12000], Training Loss: 1.4826\n",
            "Epoch [6/20], Step [2000/12000], Training Loss: 1.4809\n",
            "Epoch [6/20], Step [2500/12000], Training Loss: 1.4818\n",
            "Epoch [6/20], Step [3000/12000], Training Loss: 1.4815\n",
            "Epoch [6/20], Step [3500/12000], Training Loss: 1.4869\n",
            "Epoch [6/20], Step [4000/12000], Training Loss: 1.4868\n",
            "Epoch [6/20], Step [4500/12000], Training Loss: 1.4822\n",
            "Epoch [6/20], Step [5000/12000], Training Loss: 1.4818\n",
            "Epoch [6/20], Step [5500/12000], Training Loss: 1.4804\n",
            "Epoch [6/20], Step [6000/12000], Training Loss: 1.4830\n",
            "Epoch [6/20], Step [6500/12000], Training Loss: 1.4853\n",
            "Epoch [6/20], Step [7000/12000], Training Loss: 1.4821\n",
            "Epoch [6/20], Step [7500/12000], Training Loss: 1.4869\n",
            "Epoch [6/20], Step [8000/12000], Training Loss: 1.4819\n",
            "Epoch [6/20], Step [8500/12000], Training Loss: 1.4844\n",
            "Epoch [6/20], Step [9000/12000], Training Loss: 1.4796\n",
            "Epoch [6/20], Step [9500/12000], Training Loss: 1.4850\n",
            "Epoch [6/20], Step [10000/12000], Training Loss: 1.4867\n",
            "Epoch [6/20], Step [10500/12000], Training Loss: 1.4883\n",
            "Epoch [6/20], Step [11000/12000], Training Loss: 1.4835\n",
            "Epoch [6/20], Step [11500/12000], Training Loss: 1.4842\n",
            "Epoch [6/20], Step [12000/12000], Training Loss: 1.4820\n",
            "Epoch [7/20], Step [500/12000], Training Loss: 1.4802\n",
            "Epoch [7/20], Step [1000/12000], Training Loss: 1.4820\n",
            "Epoch [7/20], Step [1500/12000], Training Loss: 1.4815\n",
            "Epoch [7/20], Step [2000/12000], Training Loss: 1.4793\n",
            "Epoch [7/20], Step [2500/12000], Training Loss: 1.4836\n",
            "Epoch [7/20], Step [3000/12000], Training Loss: 1.4776\n",
            "Epoch [7/20], Step [3500/12000], Training Loss: 1.4805\n",
            "Epoch [7/20], Step [4000/12000], Training Loss: 1.4788\n",
            "Epoch [7/20], Step [4500/12000], Training Loss: 1.4807\n",
            "Epoch [7/20], Step [5000/12000], Training Loss: 1.4753\n",
            "Epoch [7/20], Step [5500/12000], Training Loss: 1.4819\n",
            "Epoch [7/20], Step [6000/12000], Training Loss: 1.4820\n",
            "Epoch [7/20], Step [6500/12000], Training Loss: 1.4854\n",
            "Epoch [7/20], Step [7000/12000], Training Loss: 1.4809\n",
            "Epoch [7/20], Step [7500/12000], Training Loss: 1.4789\n",
            "Epoch [7/20], Step [8000/12000], Training Loss: 1.4803\n",
            "Epoch [7/20], Step [8500/12000], Training Loss: 1.4790\n",
            "Epoch [7/20], Step [9000/12000], Training Loss: 1.4853\n",
            "Epoch [7/20], Step [9500/12000], Training Loss: 1.4793\n",
            "Epoch [7/20], Step [10000/12000], Training Loss: 1.4815\n",
            "Epoch [7/20], Step [10500/12000], Training Loss: 1.4818\n",
            "Epoch [7/20], Step [11000/12000], Training Loss: 1.4795\n",
            "Epoch [7/20], Step [11500/12000], Training Loss: 1.4801\n",
            "Epoch [7/20], Step [12000/12000], Training Loss: 1.4815\n",
            "Epoch [8/20], Step [500/12000], Training Loss: 1.4786\n",
            "Epoch [8/20], Step [1000/12000], Training Loss: 1.4843\n",
            "Epoch [8/20], Step [1500/12000], Training Loss: 1.4746\n",
            "Epoch [8/20], Step [2000/12000], Training Loss: 1.4811\n",
            "Epoch [8/20], Step [2500/12000], Training Loss: 1.4781\n",
            "Epoch [8/20], Step [3000/12000], Training Loss: 1.4843\n",
            "Epoch [8/20], Step [3500/12000], Training Loss: 1.4815\n",
            "Epoch [8/20], Step [4000/12000], Training Loss: 1.4786\n",
            "Epoch [8/20], Step [4500/12000], Training Loss: 1.4790\n",
            "Epoch [8/20], Step [5000/12000], Training Loss: 1.4777\n",
            "Epoch [8/20], Step [5500/12000], Training Loss: 1.4789\n",
            "Epoch [8/20], Step [6000/12000], Training Loss: 1.4784\n",
            "Epoch [8/20], Step [6500/12000], Training Loss: 1.4771\n",
            "Epoch [8/20], Step [7000/12000], Training Loss: 1.4783\n",
            "Epoch [8/20], Step [7500/12000], Training Loss: 1.4733\n",
            "Epoch [8/20], Step [8000/12000], Training Loss: 1.4843\n",
            "Epoch [8/20], Step [8500/12000], Training Loss: 1.4828\n",
            "Epoch [8/20], Step [9000/12000], Training Loss: 1.4768\n",
            "Epoch [8/20], Step [9500/12000], Training Loss: 1.4811\n",
            "Epoch [8/20], Step [10000/12000], Training Loss: 1.4820\n",
            "Epoch [8/20], Step [10500/12000], Training Loss: 1.4768\n",
            "Epoch [8/20], Step [11000/12000], Training Loss: 1.4836\n",
            "Epoch [8/20], Step [11500/12000], Training Loss: 1.4774\n",
            "Epoch [8/20], Step [12000/12000], Training Loss: 1.4791\n",
            "Epoch [9/20], Step [500/12000], Training Loss: 1.4780\n",
            "Epoch [9/20], Step [1000/12000], Training Loss: 1.4723\n",
            "Epoch [9/20], Step [1500/12000], Training Loss: 1.4807\n",
            "Epoch [9/20], Step [2000/12000], Training Loss: 1.4726\n",
            "Epoch [9/20], Step [2500/12000], Training Loss: 1.4739\n",
            "Epoch [9/20], Step [3000/12000], Training Loss: 1.4760\n",
            "Epoch [9/20], Step [3500/12000], Training Loss: 1.4835\n",
            "Epoch [9/20], Step [4000/12000], Training Loss: 1.4783\n",
            "Epoch [9/20], Step [4500/12000], Training Loss: 1.4832\n",
            "Epoch [9/20], Step [5000/12000], Training Loss: 1.4782\n",
            "Epoch [9/20], Step [5500/12000], Training Loss: 1.4811\n",
            "Epoch [9/20], Step [6000/12000], Training Loss: 1.4779\n",
            "Epoch [9/20], Step [6500/12000], Training Loss: 1.4800\n",
            "Epoch [9/20], Step [7000/12000], Training Loss: 1.4798\n",
            "Epoch [9/20], Step [7500/12000], Training Loss: 1.4729\n",
            "Epoch [9/20], Step [8000/12000], Training Loss: 1.4780\n",
            "Epoch [9/20], Step [8500/12000], Training Loss: 1.4790\n",
            "Epoch [9/20], Step [9000/12000], Training Loss: 1.4808\n",
            "Epoch [9/20], Step [9500/12000], Training Loss: 1.4797\n",
            "Epoch [9/20], Step [10000/12000], Training Loss: 1.4849\n",
            "Epoch [9/20], Step [10500/12000], Training Loss: 1.4764\n",
            "Epoch [9/20], Step [11000/12000], Training Loss: 1.4787\n",
            "Epoch [9/20], Step [11500/12000], Training Loss: 1.4784\n",
            "Epoch [9/20], Step [12000/12000], Training Loss: 1.4746\n",
            "Epoch [10/20], Step [500/12000], Training Loss: 1.4758\n",
            "Epoch [10/20], Step [1000/12000], Training Loss: 1.4798\n",
            "Epoch [10/20], Step [1500/12000], Training Loss: 1.4817\n",
            "Epoch [10/20], Step [2000/12000], Training Loss: 1.4778\n",
            "Epoch [10/20], Step [2500/12000], Training Loss: 1.4739\n",
            "Epoch [10/20], Step [3000/12000], Training Loss: 1.4746\n",
            "Epoch [10/20], Step [3500/12000], Training Loss: 1.4713\n",
            "Epoch [10/20], Step [4000/12000], Training Loss: 1.4763\n",
            "Epoch [10/20], Step [4500/12000], Training Loss: 1.4773\n",
            "Epoch [10/20], Step [5000/12000], Training Loss: 1.4764\n",
            "Epoch [10/20], Step [5500/12000], Training Loss: 1.4763\n",
            "Epoch [10/20], Step [6000/12000], Training Loss: 1.4818\n",
            "Epoch [10/20], Step [6500/12000], Training Loss: 1.4772\n",
            "Epoch [10/20], Step [7000/12000], Training Loss: 1.4733\n",
            "Epoch [10/20], Step [7500/12000], Training Loss: 1.4801\n",
            "Epoch [10/20], Step [8000/12000], Training Loss: 1.4740\n",
            "Epoch [10/20], Step [8500/12000], Training Loss: 1.4742\n",
            "Epoch [10/20], Step [9000/12000], Training Loss: 1.4776\n",
            "Epoch [10/20], Step [9500/12000], Training Loss: 1.4798\n",
            "Epoch [10/20], Step [10000/12000], Training Loss: 1.4769\n",
            "Epoch [10/20], Step [10500/12000], Training Loss: 1.4787\n",
            "Epoch [10/20], Step [11000/12000], Training Loss: 1.4811\n",
            "Epoch [10/20], Step [11500/12000], Training Loss: 1.4790\n",
            "Epoch [10/20], Step [12000/12000], Training Loss: 1.4793\n",
            "Epoch [11/20], Step [500/12000], Training Loss: 1.4767\n",
            "Epoch [11/20], Step [1000/12000], Training Loss: 1.4769\n",
            "Epoch [11/20], Step [1500/12000], Training Loss: 1.4734\n",
            "Epoch [11/20], Step [2000/12000], Training Loss: 1.4783\n",
            "Epoch [11/20], Step [2500/12000], Training Loss: 1.4715\n",
            "Epoch [11/20], Step [3000/12000], Training Loss: 1.4783\n",
            "Epoch [11/20], Step [3500/12000], Training Loss: 1.4767\n",
            "Epoch [11/20], Step [4000/12000], Training Loss: 1.4751\n",
            "Epoch [11/20], Step [4500/12000], Training Loss: 1.4755\n",
            "Epoch [11/20], Step [5000/12000], Training Loss: 1.4768\n",
            "Epoch [11/20], Step [5500/12000], Training Loss: 1.4735\n",
            "Epoch [11/20], Step [6000/12000], Training Loss: 1.4802\n",
            "Epoch [11/20], Step [6500/12000], Training Loss: 1.4792\n",
            "Epoch [11/20], Step [7000/12000], Training Loss: 1.4750\n",
            "Epoch [11/20], Step [7500/12000], Training Loss: 1.4778\n",
            "Epoch [11/20], Step [8000/12000], Training Loss: 1.4760\n",
            "Epoch [11/20], Step [8500/12000], Training Loss: 1.4740\n",
            "Epoch [11/20], Step [9000/12000], Training Loss: 1.4765\n",
            "Epoch [11/20], Step [9500/12000], Training Loss: 1.4753\n",
            "Epoch [11/20], Step [10000/12000], Training Loss: 1.4741\n",
            "Epoch [11/20], Step [10500/12000], Training Loss: 1.4732\n",
            "Epoch [11/20], Step [11000/12000], Training Loss: 1.4756\n",
            "Epoch [11/20], Step [11500/12000], Training Loss: 1.4753\n",
            "Epoch [11/20], Step [12000/12000], Training Loss: 1.4760\n",
            "Epoch [12/20], Step [500/12000], Training Loss: 1.4728\n",
            "Epoch [12/20], Step [1000/12000], Training Loss: 1.4741\n",
            "Epoch [12/20], Step [1500/12000], Training Loss: 1.4761\n",
            "Epoch [12/20], Step [2000/12000], Training Loss: 1.4748\n",
            "Epoch [12/20], Step [2500/12000], Training Loss: 1.4753\n",
            "Epoch [12/20], Step [3000/12000], Training Loss: 1.4743\n",
            "Epoch [12/20], Step [3500/12000], Training Loss: 1.4749\n",
            "Epoch [12/20], Step [4000/12000], Training Loss: 1.4744\n",
            "Epoch [12/20], Step [4500/12000], Training Loss: 1.4767\n",
            "Epoch [12/20], Step [5000/12000], Training Loss: 1.4749\n",
            "Epoch [12/20], Step [5500/12000], Training Loss: 1.4799\n",
            "Epoch [12/20], Step [6000/12000], Training Loss: 1.4763\n",
            "Epoch [12/20], Step [6500/12000], Training Loss: 1.4733\n",
            "Epoch [12/20], Step [7000/12000], Training Loss: 1.4749\n",
            "Epoch [12/20], Step [7500/12000], Training Loss: 1.4741\n",
            "Epoch [12/20], Step [8000/12000], Training Loss: 1.4748\n",
            "Epoch [12/20], Step [8500/12000], Training Loss: 1.4722\n",
            "Epoch [12/20], Step [9000/12000], Training Loss: 1.4782\n",
            "Epoch [12/20], Step [9500/12000], Training Loss: 1.4748\n",
            "Epoch [12/20], Step [10000/12000], Training Loss: 1.4724\n",
            "Epoch [12/20], Step [10500/12000], Training Loss: 1.4781\n",
            "Epoch [12/20], Step [11000/12000], Training Loss: 1.4772\n",
            "Epoch [12/20], Step [11500/12000], Training Loss: 1.4741\n",
            "Epoch [12/20], Step [12000/12000], Training Loss: 1.4759\n",
            "Epoch [13/20], Step [500/12000], Training Loss: 1.4740\n",
            "Epoch [13/20], Step [1000/12000], Training Loss: 1.4719\n",
            "Epoch [13/20], Step [1500/12000], Training Loss: 1.4743\n",
            "Epoch [13/20], Step [2000/12000], Training Loss: 1.4773\n",
            "Epoch [13/20], Step [2500/12000], Training Loss: 1.4794\n",
            "Epoch [13/20], Step [3000/12000], Training Loss: 1.4772\n",
            "Epoch [13/20], Step [3500/12000], Training Loss: 1.4703\n",
            "Epoch [13/20], Step [4000/12000], Training Loss: 1.4739\n",
            "Epoch [13/20], Step [4500/12000], Training Loss: 1.4735\n",
            "Epoch [13/20], Step [5000/12000], Training Loss: 1.4720\n",
            "Epoch [13/20], Step [5500/12000], Training Loss: 1.4731\n",
            "Epoch [13/20], Step [6000/12000], Training Loss: 1.4733\n",
            "Epoch [13/20], Step [6500/12000], Training Loss: 1.4747\n",
            "Epoch [13/20], Step [7000/12000], Training Loss: 1.4760\n",
            "Epoch [13/20], Step [7500/12000], Training Loss: 1.4759\n",
            "Epoch [13/20], Step [8000/12000], Training Loss: 1.4730\n",
            "Epoch [13/20], Step [8500/12000], Training Loss: 1.4763\n",
            "Epoch [13/20], Step [9000/12000], Training Loss: 1.4765\n",
            "Epoch [13/20], Step [9500/12000], Training Loss: 1.4746\n",
            "Epoch [13/20], Step [10000/12000], Training Loss: 1.4794\n",
            "Epoch [13/20], Step [10500/12000], Training Loss: 1.4772\n",
            "Epoch [13/20], Step [11000/12000], Training Loss: 1.4720\n",
            "Epoch [13/20], Step [11500/12000], Training Loss: 1.4755\n",
            "Epoch [13/20], Step [12000/12000], Training Loss: 1.4723\n",
            "Epoch [14/20], Step [500/12000], Training Loss: 1.4717\n",
            "Epoch [14/20], Step [1000/12000], Training Loss: 1.4698\n",
            "Epoch [14/20], Step [1500/12000], Training Loss: 1.4739\n",
            "Epoch [14/20], Step [2000/12000], Training Loss: 1.4734\n",
            "Epoch [14/20], Step [2500/12000], Training Loss: 1.4773\n",
            "Epoch [14/20], Step [3000/12000], Training Loss: 1.4753\n",
            "Epoch [14/20], Step [3500/12000], Training Loss: 1.4702\n",
            "Epoch [14/20], Step [4000/12000], Training Loss: 1.4705\n",
            "Epoch [14/20], Step [4500/12000], Training Loss: 1.4757\n",
            "Epoch [14/20], Step [5000/12000], Training Loss: 1.4753\n",
            "Epoch [14/20], Step [5500/12000], Training Loss: 1.4742\n",
            "Epoch [14/20], Step [6000/12000], Training Loss: 1.4732\n",
            "Epoch [14/20], Step [6500/12000], Training Loss: 1.4771\n",
            "Epoch [14/20], Step [7000/12000], Training Loss: 1.4781\n",
            "Epoch [14/20], Step [7500/12000], Training Loss: 1.4753\n",
            "Epoch [14/20], Step [8000/12000], Training Loss: 1.4775\n",
            "Epoch [14/20], Step [8500/12000], Training Loss: 1.4752\n",
            "Epoch [14/20], Step [9000/12000], Training Loss: 1.4722\n",
            "Epoch [14/20], Step [9500/12000], Training Loss: 1.4727\n",
            "Epoch [14/20], Step [10000/12000], Training Loss: 1.4721\n",
            "Epoch [14/20], Step [10500/12000], Training Loss: 1.4758\n",
            "Epoch [14/20], Step [11000/12000], Training Loss: 1.4764\n",
            "Epoch [14/20], Step [11500/12000], Training Loss: 1.4759\n",
            "Epoch [14/20], Step [12000/12000], Training Loss: 1.4732\n",
            "Epoch [15/20], Step [500/12000], Training Loss: 1.4743\n",
            "Epoch [15/20], Step [1000/12000], Training Loss: 1.4704\n",
            "Epoch [15/20], Step [1500/12000], Training Loss: 1.4711\n",
            "Epoch [15/20], Step [2000/12000], Training Loss: 1.4671\n",
            "Epoch [15/20], Step [2500/12000], Training Loss: 1.4741\n",
            "Epoch [15/20], Step [3000/12000], Training Loss: 1.4748\n",
            "Epoch [15/20], Step [3500/12000], Training Loss: 1.4737\n",
            "Epoch [15/20], Step [4000/12000], Training Loss: 1.4722\n",
            "Epoch [15/20], Step [4500/12000], Training Loss: 1.4726\n",
            "Epoch [15/20], Step [5000/12000], Training Loss: 1.4726\n",
            "Epoch [15/20], Step [5500/12000], Training Loss: 1.4766\n",
            "Epoch [15/20], Step [6000/12000], Training Loss: 1.4743\n",
            "Epoch [15/20], Step [6500/12000], Training Loss: 1.4751\n",
            "Epoch [15/20], Step [7000/12000], Training Loss: 1.4754\n",
            "Epoch [15/20], Step [7500/12000], Training Loss: 1.4740\n",
            "Epoch [15/20], Step [8000/12000], Training Loss: 1.4713\n",
            "Epoch [15/20], Step [8500/12000], Training Loss: 1.4734\n",
            "Epoch [15/20], Step [9000/12000], Training Loss: 1.4739\n",
            "Epoch [15/20], Step [9500/12000], Training Loss: 1.4725\n",
            "Epoch [15/20], Step [10000/12000], Training Loss: 1.4718\n",
            "Epoch [15/20], Step [10500/12000], Training Loss: 1.4744\n",
            "Epoch [15/20], Step [11000/12000], Training Loss: 1.4749\n",
            "Epoch [15/20], Step [11500/12000], Training Loss: 1.4739\n",
            "Epoch [15/20], Step [12000/12000], Training Loss: 1.4772\n",
            "Epoch [16/20], Step [500/12000], Training Loss: 1.4747\n",
            "Epoch [16/20], Step [1000/12000], Training Loss: 1.4696\n",
            "Epoch [16/20], Step [1500/12000], Training Loss: 1.4716\n",
            "Epoch [16/20], Step [2000/12000], Training Loss: 1.4774\n",
            "Epoch [16/20], Step [2500/12000], Training Loss: 1.4712\n",
            "Epoch [16/20], Step [3000/12000], Training Loss: 1.4730\n",
            "Epoch [16/20], Step [3500/12000], Training Loss: 1.4670\n",
            "Epoch [16/20], Step [4000/12000], Training Loss: 1.4746\n",
            "Epoch [16/20], Step [4500/12000], Training Loss: 1.4746\n",
            "Epoch [16/20], Step [5000/12000], Training Loss: 1.4712\n",
            "Epoch [16/20], Step [5500/12000], Training Loss: 1.4726\n",
            "Epoch [16/20], Step [6000/12000], Training Loss: 1.4747\n",
            "Epoch [16/20], Step [6500/12000], Training Loss: 1.4771\n",
            "Epoch [16/20], Step [7000/12000], Training Loss: 1.4761\n",
            "Epoch [16/20], Step [7500/12000], Training Loss: 1.4721\n",
            "Epoch [16/20], Step [8000/12000], Training Loss: 1.4724\n",
            "Epoch [16/20], Step [8500/12000], Training Loss: 1.4711\n",
            "Epoch [16/20], Step [9000/12000], Training Loss: 1.4710\n",
            "Epoch [16/20], Step [9500/12000], Training Loss: 1.4714\n",
            "Epoch [16/20], Step [10000/12000], Training Loss: 1.4698\n",
            "Epoch [16/20], Step [10500/12000], Training Loss: 1.4763\n",
            "Epoch [16/20], Step [11000/12000], Training Loss: 1.4750\n",
            "Epoch [16/20], Step [11500/12000], Training Loss: 1.4722\n",
            "Epoch [16/20], Step [12000/12000], Training Loss: 1.4736\n",
            "Epoch [17/20], Step [500/12000], Training Loss: 1.4755\n",
            "Epoch [17/20], Step [1000/12000], Training Loss: 1.4713\n",
            "Epoch [17/20], Step [1500/12000], Training Loss: 1.4730\n",
            "Epoch [17/20], Step [2000/12000], Training Loss: 1.4724\n",
            "Epoch [17/20], Step [2500/12000], Training Loss: 1.4723\n",
            "Epoch [17/20], Step [3000/12000], Training Loss: 1.4699\n",
            "Epoch [17/20], Step [3500/12000], Training Loss: 1.4670\n",
            "Epoch [17/20], Step [4000/12000], Training Loss: 1.4723\n",
            "Epoch [17/20], Step [4500/12000], Training Loss: 1.4728\n",
            "Epoch [17/20], Step [5000/12000], Training Loss: 1.4719\n",
            "Epoch [17/20], Step [5500/12000], Training Loss: 1.4725\n",
            "Epoch [17/20], Step [6000/12000], Training Loss: 1.4750\n",
            "Epoch [17/20], Step [6500/12000], Training Loss: 1.4756\n",
            "Epoch [17/20], Step [7000/12000], Training Loss: 1.4723\n",
            "Epoch [17/20], Step [7500/12000], Training Loss: 1.4716\n",
            "Epoch [17/20], Step [8000/12000], Training Loss: 1.4745\n",
            "Epoch [17/20], Step [8500/12000], Training Loss: 1.4735\n",
            "Epoch [17/20], Step [9000/12000], Training Loss: 1.4698\n",
            "Epoch [17/20], Step [9500/12000], Training Loss: 1.4706\n",
            "Epoch [17/20], Step [10000/12000], Training Loss: 1.4714\n",
            "Epoch [17/20], Step [10500/12000], Training Loss: 1.4707\n",
            "Epoch [17/20], Step [11000/12000], Training Loss: 1.4747\n",
            "Epoch [17/20], Step [11500/12000], Training Loss: 1.4696\n",
            "Epoch [17/20], Step [12000/12000], Training Loss: 1.4698\n",
            "Epoch [18/20], Step [500/12000], Training Loss: 1.4709\n",
            "Epoch [18/20], Step [1000/12000], Training Loss: 1.4692\n",
            "Epoch [18/20], Step [1500/12000], Training Loss: 1.4685\n",
            "Epoch [18/20], Step [2000/12000], Training Loss: 1.4719\n",
            "Epoch [18/20], Step [2500/12000], Training Loss: 1.4731\n",
            "Epoch [18/20], Step [3000/12000], Training Loss: 1.4687\n",
            "Epoch [18/20], Step [3500/12000], Training Loss: 1.4705\n",
            "Epoch [18/20], Step [4000/12000], Training Loss: 1.4688\n",
            "Epoch [18/20], Step [4500/12000], Training Loss: 1.4695\n",
            "Epoch [18/20], Step [5000/12000], Training Loss: 1.4703\n",
            "Epoch [18/20], Step [5500/12000], Training Loss: 1.4701\n",
            "Epoch [18/20], Step [6000/12000], Training Loss: 1.4735\n",
            "Epoch [18/20], Step [6500/12000], Training Loss: 1.4720\n",
            "Epoch [18/20], Step [7000/12000], Training Loss: 1.4726\n",
            "Epoch [18/20], Step [7500/12000], Training Loss: 1.4720\n",
            "Epoch [18/20], Step [8000/12000], Training Loss: 1.4752\n",
            "Epoch [18/20], Step [8500/12000], Training Loss: 1.4755\n",
            "Epoch [18/20], Step [9000/12000], Training Loss: 1.4720\n",
            "Epoch [18/20], Step [9500/12000], Training Loss: 1.4699\n",
            "Epoch [18/20], Step [10000/12000], Training Loss: 1.4711\n",
            "Epoch [18/20], Step [10500/12000], Training Loss: 1.4714\n",
            "Epoch [18/20], Step [11000/12000], Training Loss: 1.4721\n",
            "Epoch [18/20], Step [11500/12000], Training Loss: 1.4749\n",
            "Epoch [18/20], Step [12000/12000], Training Loss: 1.4742\n",
            "Epoch [19/20], Step [500/12000], Training Loss: 1.4732\n",
            "Epoch [19/20], Step [1000/12000], Training Loss: 1.4706\n",
            "Epoch [19/20], Step [1500/12000], Training Loss: 1.4695\n",
            "Epoch [19/20], Step [2000/12000], Training Loss: 1.4705\n",
            "Epoch [19/20], Step [2500/12000], Training Loss: 1.4737\n",
            "Epoch [19/20], Step [3000/12000], Training Loss: 1.4723\n",
            "Epoch [19/20], Step [3500/12000], Training Loss: 1.4709\n",
            "Epoch [19/20], Step [4000/12000], Training Loss: 1.4708\n",
            "Epoch [19/20], Step [4500/12000], Training Loss: 1.4714\n",
            "Epoch [19/20], Step [5000/12000], Training Loss: 1.4718\n",
            "Epoch [19/20], Step [5500/12000], Training Loss: 1.4706\n",
            "Epoch [19/20], Step [6000/12000], Training Loss: 1.4688\n",
            "Epoch [19/20], Step [6500/12000], Training Loss: 1.4718\n",
            "Epoch [19/20], Step [7000/12000], Training Loss: 1.4697\n",
            "Epoch [19/20], Step [7500/12000], Training Loss: 1.4738\n",
            "Epoch [19/20], Step [8000/12000], Training Loss: 1.4736\n",
            "Epoch [19/20], Step [8500/12000], Training Loss: 1.4695\n",
            "Epoch [19/20], Step [9000/12000], Training Loss: 1.4695\n",
            "Epoch [19/20], Step [9500/12000], Training Loss: 1.4749\n",
            "Epoch [19/20], Step [10000/12000], Training Loss: 1.4727\n",
            "Epoch [19/20], Step [10500/12000], Training Loss: 1.4724\n",
            "Epoch [19/20], Step [11000/12000], Training Loss: 1.4714\n",
            "Epoch [19/20], Step [11500/12000], Training Loss: 1.4693\n",
            "Epoch [19/20], Step [12000/12000], Training Loss: 1.4750\n",
            "Epoch [20/20], Step [500/12000], Training Loss: 1.4696\n",
            "Epoch [20/20], Step [1000/12000], Training Loss: 1.4681\n",
            "Epoch [20/20], Step [1500/12000], Training Loss: 1.4687\n",
            "Epoch [20/20], Step [2000/12000], Training Loss: 1.4718\n",
            "Epoch [20/20], Step [2500/12000], Training Loss: 1.4723\n",
            "Epoch [20/20], Step [3000/12000], Training Loss: 1.4691\n",
            "Epoch [20/20], Step [3500/12000], Training Loss: 1.4714\n",
            "Epoch [20/20], Step [4000/12000], Training Loss: 1.4731\n",
            "Epoch [20/20], Step [4500/12000], Training Loss: 1.4702\n",
            "Epoch [20/20], Step [5000/12000], Training Loss: 1.4719\n",
            "Epoch [20/20], Step [5500/12000], Training Loss: 1.4705\n",
            "Epoch [20/20], Step [6000/12000], Training Loss: 1.4697\n",
            "Epoch [20/20], Step [6500/12000], Training Loss: 1.4696\n",
            "Epoch [20/20], Step [7000/12000], Training Loss: 1.4722\n",
            "Epoch [20/20], Step [7500/12000], Training Loss: 1.4712\n",
            "Epoch [20/20], Step [8000/12000], Training Loss: 1.4714\n",
            "Epoch [20/20], Step [8500/12000], Training Loss: 1.4721\n",
            "Epoch [20/20], Step [9000/12000], Training Loss: 1.4691\n",
            "Epoch [20/20], Step [9500/12000], Training Loss: 1.4721\n",
            "Epoch [20/20], Step [10000/12000], Training Loss: 1.4738\n",
            "Epoch [20/20], Step [10500/12000], Training Loss: 1.4728\n",
            "Epoch [20/20], Step [11000/12000], Training Loss: 1.4715\n",
            "Epoch [20/20], Step [11500/12000], Training Loss: 1.4723\n",
            "Epoch [20/20], Step [12000/12000], Training Loss: 1.4701\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        images, digits = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer1.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model1(images)\n",
        "        loss = criterion(outputs, digits)\n",
        "        loss.backward()\n",
        "        optimizer1.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 500 == 499:    # print every 500 mini-batches\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Training Loss: {:.4f}' \n",
        "                .format(epoch+1, num_epochs, i+1, total_step, running_loss/500))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GS9137UiRCMd"
      },
      "source": [
        "CNN2 - Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZbdqkQO2RFZy",
        "outputId": "884ae8e6-9fff-4a01-cf6f-921961434093"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/20], Step [500/12000], Training Loss: 2.2990\n",
            "Epoch [1/20], Step [1000/12000], Training Loss: 2.2277\n",
            "Epoch [1/20], Step [1500/12000], Training Loss: 1.7859\n",
            "Epoch [1/20], Step [2000/12000], Training Loss: 1.6738\n",
            "Epoch [1/20], Step [2500/12000], Training Loss: 1.6594\n",
            "Epoch [1/20], Step [3000/12000], Training Loss: 1.6498\n",
            "Epoch [1/20], Step [3500/12000], Training Loss: 1.6419\n",
            "Epoch [1/20], Step [4000/12000], Training Loss: 1.6347\n",
            "Epoch [1/20], Step [4500/12000], Training Loss: 1.6257\n",
            "Epoch [1/20], Step [5000/12000], Training Loss: 1.6292\n",
            "Epoch [1/20], Step [5500/12000], Training Loss: 1.6364\n",
            "Epoch [1/20], Step [6000/12000], Training Loss: 1.6122\n",
            "Epoch [1/20], Step [6500/12000], Training Loss: 1.6061\n",
            "Epoch [1/20], Step [7000/12000], Training Loss: 1.6047\n",
            "Epoch [1/20], Step [7500/12000], Training Loss: 1.6086\n",
            "Epoch [1/20], Step [8000/12000], Training Loss: 1.6002\n",
            "Epoch [1/20], Step [8500/12000], Training Loss: 1.6013\n",
            "Epoch [1/20], Step [9000/12000], Training Loss: 1.6031\n",
            "Epoch [1/20], Step [9500/12000], Training Loss: 1.5927\n",
            "Epoch [1/20], Step [10000/12000], Training Loss: 1.5989\n",
            "Epoch [1/20], Step [10500/12000], Training Loss: 1.5865\n",
            "Epoch [1/20], Step [11000/12000], Training Loss: 1.5996\n",
            "Epoch [1/20], Step [11500/12000], Training Loss: 1.5907\n",
            "Epoch [1/20], Step [12000/12000], Training Loss: 1.5967\n",
            "Epoch [2/20], Step [500/12000], Training Loss: 1.5769\n",
            "Epoch [2/20], Step [1000/12000], Training Loss: 1.5755\n",
            "Epoch [2/20], Step [1500/12000], Training Loss: 1.5859\n",
            "Epoch [2/20], Step [2000/12000], Training Loss: 1.5920\n",
            "Epoch [2/20], Step [2500/12000], Training Loss: 1.5903\n",
            "Epoch [2/20], Step [3000/12000], Training Loss: 1.5828\n",
            "Epoch [2/20], Step [3500/12000], Training Loss: 1.5347\n",
            "Epoch [2/20], Step [4000/12000], Training Loss: 1.5175\n",
            "Epoch [2/20], Step [4500/12000], Training Loss: 1.5079\n",
            "Epoch [2/20], Step [5000/12000], Training Loss: 1.5078\n",
            "Epoch [2/20], Step [5500/12000], Training Loss: 1.5041\n",
            "Epoch [2/20], Step [6000/12000], Training Loss: 1.5038\n",
            "Epoch [2/20], Step [6500/12000], Training Loss: 1.5115\n",
            "Epoch [2/20], Step [7000/12000], Training Loss: 1.4996\n",
            "Epoch [2/20], Step [7500/12000], Training Loss: 1.5037\n",
            "Epoch [2/20], Step [8000/12000], Training Loss: 1.5037\n",
            "Epoch [2/20], Step [8500/12000], Training Loss: 1.4990\n",
            "Epoch [2/20], Step [9000/12000], Training Loss: 1.4952\n",
            "Epoch [2/20], Step [9500/12000], Training Loss: 1.5011\n",
            "Epoch [2/20], Step [10000/12000], Training Loss: 1.4962\n",
            "Epoch [2/20], Step [10500/12000], Training Loss: 1.4950\n",
            "Epoch [2/20], Step [11000/12000], Training Loss: 1.4992\n",
            "Epoch [2/20], Step [11500/12000], Training Loss: 1.4997\n",
            "Epoch [2/20], Step [12000/12000], Training Loss: 1.4954\n",
            "Epoch [3/20], Step [500/12000], Training Loss: 1.4977\n",
            "Epoch [3/20], Step [1000/12000], Training Loss: 1.4926\n",
            "Epoch [3/20], Step [1500/12000], Training Loss: 1.4862\n",
            "Epoch [3/20], Step [2000/12000], Training Loss: 1.4980\n",
            "Epoch [3/20], Step [2500/12000], Training Loss: 1.4919\n",
            "Epoch [3/20], Step [3000/12000], Training Loss: 1.4924\n",
            "Epoch [3/20], Step [3500/12000], Training Loss: 1.4929\n",
            "Epoch [3/20], Step [4000/12000], Training Loss: 1.4937\n",
            "Epoch [3/20], Step [4500/12000], Training Loss: 1.4872\n",
            "Epoch [3/20], Step [5000/12000], Training Loss: 1.4858\n",
            "Epoch [3/20], Step [5500/12000], Training Loss: 1.4933\n",
            "Epoch [3/20], Step [6000/12000], Training Loss: 1.4951\n",
            "Epoch [3/20], Step [6500/12000], Training Loss: 1.4900\n",
            "Epoch [3/20], Step [7000/12000], Training Loss: 1.4864\n",
            "Epoch [3/20], Step [7500/12000], Training Loss: 1.4913\n",
            "Epoch [3/20], Step [8000/12000], Training Loss: 1.4855\n",
            "Epoch [3/20], Step [8500/12000], Training Loss: 1.4920\n",
            "Epoch [3/20], Step [9000/12000], Training Loss: 1.4937\n",
            "Epoch [3/20], Step [9500/12000], Training Loss: 1.4829\n",
            "Epoch [3/20], Step [10000/12000], Training Loss: 1.4869\n",
            "Epoch [3/20], Step [10500/12000], Training Loss: 1.4970\n",
            "Epoch [3/20], Step [11000/12000], Training Loss: 1.4862\n",
            "Epoch [3/20], Step [11500/12000], Training Loss: 1.4854\n",
            "Epoch [3/20], Step [12000/12000], Training Loss: 1.4871\n",
            "Epoch [4/20], Step [500/12000], Training Loss: 1.4876\n",
            "Epoch [4/20], Step [1000/12000], Training Loss: 1.4861\n",
            "Epoch [4/20], Step [1500/12000], Training Loss: 1.4830\n",
            "Epoch [4/20], Step [2000/12000], Training Loss: 1.4839\n",
            "Epoch [4/20], Step [2500/12000], Training Loss: 1.4813\n",
            "Epoch [4/20], Step [3000/12000], Training Loss: 1.4789\n",
            "Epoch [4/20], Step [3500/12000], Training Loss: 1.4814\n",
            "Epoch [4/20], Step [4000/12000], Training Loss: 1.4808\n",
            "Epoch [4/20], Step [4500/12000], Training Loss: 1.4866\n",
            "Epoch [4/20], Step [5000/12000], Training Loss: 1.4833\n",
            "Epoch [4/20], Step [5500/12000], Training Loss: 1.4851\n",
            "Epoch [4/20], Step [6000/12000], Training Loss: 1.4774\n",
            "Epoch [4/20], Step [6500/12000], Training Loss: 1.4880\n",
            "Epoch [4/20], Step [7000/12000], Training Loss: 1.4876\n",
            "Epoch [4/20], Step [7500/12000], Training Loss: 1.4819\n",
            "Epoch [4/20], Step [8000/12000], Training Loss: 1.4846\n",
            "Epoch [4/20], Step [8500/12000], Training Loss: 1.4873\n",
            "Epoch [4/20], Step [9000/12000], Training Loss: 1.4851\n",
            "Epoch [4/20], Step [9500/12000], Training Loss: 1.4899\n",
            "Epoch [4/20], Step [10000/12000], Training Loss: 1.4888\n",
            "Epoch [4/20], Step [10500/12000], Training Loss: 1.4820\n",
            "Epoch [4/20], Step [11000/12000], Training Loss: 1.4843\n",
            "Epoch [4/20], Step [11500/12000], Training Loss: 1.4850\n",
            "Epoch [4/20], Step [12000/12000], Training Loss: 1.4886\n",
            "Epoch [5/20], Step [500/12000], Training Loss: 1.4841\n",
            "Epoch [5/20], Step [1000/12000], Training Loss: 1.4792\n",
            "Epoch [5/20], Step [1500/12000], Training Loss: 1.4785\n",
            "Epoch [5/20], Step [2000/12000], Training Loss: 1.4811\n",
            "Epoch [5/20], Step [2500/12000], Training Loss: 1.4875\n",
            "Epoch [5/20], Step [3000/12000], Training Loss: 1.4864\n",
            "Epoch [5/20], Step [3500/12000], Training Loss: 1.4826\n",
            "Epoch [5/20], Step [4000/12000], Training Loss: 1.4788\n",
            "Epoch [5/20], Step [4500/12000], Training Loss: 1.4800\n",
            "Epoch [5/20], Step [5000/12000], Training Loss: 1.4757\n",
            "Epoch [5/20], Step [5500/12000], Training Loss: 1.4804\n",
            "Epoch [5/20], Step [6000/12000], Training Loss: 1.4771\n",
            "Epoch [5/20], Step [6500/12000], Training Loss: 1.4787\n",
            "Epoch [5/20], Step [7000/12000], Training Loss: 1.4786\n",
            "Epoch [5/20], Step [7500/12000], Training Loss: 1.4863\n",
            "Epoch [5/20], Step [8000/12000], Training Loss: 1.4799\n",
            "Epoch [5/20], Step [8500/12000], Training Loss: 1.4752\n",
            "Epoch [5/20], Step [9000/12000], Training Loss: 1.4791\n",
            "Epoch [5/20], Step [9500/12000], Training Loss: 1.4786\n",
            "Epoch [5/20], Step [10000/12000], Training Loss: 1.4896\n",
            "Epoch [5/20], Step [10500/12000], Training Loss: 1.4833\n",
            "Epoch [5/20], Step [11000/12000], Training Loss: 1.4840\n",
            "Epoch [5/20], Step [11500/12000], Training Loss: 1.4727\n",
            "Epoch [5/20], Step [12000/12000], Training Loss: 1.4832\n",
            "Epoch [6/20], Step [500/12000], Training Loss: 1.4786\n",
            "Epoch [6/20], Step [1000/12000], Training Loss: 1.4740\n",
            "Epoch [6/20], Step [1500/12000], Training Loss: 1.4762\n",
            "Epoch [6/20], Step [2000/12000], Training Loss: 1.4790\n",
            "Epoch [6/20], Step [2500/12000], Training Loss: 1.4786\n",
            "Epoch [6/20], Step [3000/12000], Training Loss: 1.4792\n",
            "Epoch [6/20], Step [3500/12000], Training Loss: 1.4808\n",
            "Epoch [6/20], Step [4000/12000], Training Loss: 1.4768\n",
            "Epoch [6/20], Step [4500/12000], Training Loss: 1.4763\n",
            "Epoch [6/20], Step [5000/12000], Training Loss: 1.4767\n",
            "Epoch [6/20], Step [5500/12000], Training Loss: 1.4774\n",
            "Epoch [6/20], Step [6000/12000], Training Loss: 1.4805\n",
            "Epoch [6/20], Step [6500/12000], Training Loss: 1.4728\n",
            "Epoch [6/20], Step [7000/12000], Training Loss: 1.4776\n",
            "Epoch [6/20], Step [7500/12000], Training Loss: 1.4748\n",
            "Epoch [6/20], Step [8000/12000], Training Loss: 1.4793\n",
            "Epoch [6/20], Step [8500/12000], Training Loss: 1.4794\n",
            "Epoch [6/20], Step [9000/12000], Training Loss: 1.4812\n",
            "Epoch [6/20], Step [9500/12000], Training Loss: 1.4791\n",
            "Epoch [6/20], Step [10000/12000], Training Loss: 1.4788\n",
            "Epoch [6/20], Step [10500/12000], Training Loss: 1.4847\n",
            "Epoch [6/20], Step [11000/12000], Training Loss: 1.4744\n",
            "Epoch [6/20], Step [11500/12000], Training Loss: 1.4733\n",
            "Epoch [6/20], Step [12000/12000], Training Loss: 1.4807\n",
            "Epoch [7/20], Step [500/12000], Training Loss: 1.4762\n",
            "Epoch [7/20], Step [1000/12000], Training Loss: 1.4732\n",
            "Epoch [7/20], Step [1500/12000], Training Loss: 1.4754\n",
            "Epoch [7/20], Step [2000/12000], Training Loss: 1.4775\n",
            "Epoch [7/20], Step [2500/12000], Training Loss: 1.4783\n",
            "Epoch [7/20], Step [3000/12000], Training Loss: 1.4747\n",
            "Epoch [7/20], Step [3500/12000], Training Loss: 1.4811\n",
            "Epoch [7/20], Step [4000/12000], Training Loss: 1.4778\n",
            "Epoch [7/20], Step [4500/12000], Training Loss: 1.4757\n",
            "Epoch [7/20], Step [5000/12000], Training Loss: 1.4816\n",
            "Epoch [7/20], Step [5500/12000], Training Loss: 1.4739\n",
            "Epoch [7/20], Step [6000/12000], Training Loss: 1.4766\n",
            "Epoch [7/20], Step [6500/12000], Training Loss: 1.4733\n",
            "Epoch [7/20], Step [7000/12000], Training Loss: 1.4795\n",
            "Epoch [7/20], Step [7500/12000], Training Loss: 1.4722\n",
            "Epoch [7/20], Step [8000/12000], Training Loss: 1.4744\n",
            "Epoch [7/20], Step [8500/12000], Training Loss: 1.4743\n",
            "Epoch [7/20], Step [9000/12000], Training Loss: 1.4752\n",
            "Epoch [7/20], Step [9500/12000], Training Loss: 1.4720\n",
            "Epoch [7/20], Step [10000/12000], Training Loss: 1.4743\n",
            "Epoch [7/20], Step [10500/12000], Training Loss: 1.4770\n",
            "Epoch [7/20], Step [11000/12000], Training Loss: 1.4759\n",
            "Epoch [7/20], Step [11500/12000], Training Loss: 1.4737\n",
            "Epoch [7/20], Step [12000/12000], Training Loss: 1.4778\n",
            "Epoch [8/20], Step [500/12000], Training Loss: 1.4760\n",
            "Epoch [8/20], Step [1000/12000], Training Loss: 1.4755\n",
            "Epoch [8/20], Step [1500/12000], Training Loss: 1.4792\n",
            "Epoch [8/20], Step [2000/12000], Training Loss: 1.4711\n",
            "Epoch [8/20], Step [2500/12000], Training Loss: 1.4743\n",
            "Epoch [8/20], Step [3000/12000], Training Loss: 1.4706\n",
            "Epoch [8/20], Step [3500/12000], Training Loss: 1.4752\n",
            "Epoch [8/20], Step [4000/12000], Training Loss: 1.4793\n",
            "Epoch [8/20], Step [4500/12000], Training Loss: 1.4785\n",
            "Epoch [8/20], Step [5000/12000], Training Loss: 1.4778\n",
            "Epoch [8/20], Step [5500/12000], Training Loss: 1.4714\n",
            "Epoch [8/20], Step [6000/12000], Training Loss: 1.4700\n",
            "Epoch [8/20], Step [6500/12000], Training Loss: 1.4769\n",
            "Epoch [8/20], Step [7000/12000], Training Loss: 1.4793\n",
            "Epoch [8/20], Step [7500/12000], Training Loss: 1.4710\n",
            "Epoch [8/20], Step [8000/12000], Training Loss: 1.4778\n",
            "Epoch [8/20], Step [8500/12000], Training Loss: 1.4704\n",
            "Epoch [8/20], Step [9000/12000], Training Loss: 1.4767\n",
            "Epoch [8/20], Step [9500/12000], Training Loss: 1.4743\n",
            "Epoch [8/20], Step [10000/12000], Training Loss: 1.4722\n",
            "Epoch [8/20], Step [10500/12000], Training Loss: 1.4755\n",
            "Epoch [8/20], Step [11000/12000], Training Loss: 1.4725\n",
            "Epoch [8/20], Step [11500/12000], Training Loss: 1.4728\n",
            "Epoch [8/20], Step [12000/12000], Training Loss: 1.4783\n",
            "Epoch [9/20], Step [500/12000], Training Loss: 1.4727\n",
            "Epoch [9/20], Step [1000/12000], Training Loss: 1.4755\n",
            "Epoch [9/20], Step [1500/12000], Training Loss: 1.4713\n",
            "Epoch [9/20], Step [2000/12000], Training Loss: 1.4766\n",
            "Epoch [9/20], Step [2500/12000], Training Loss: 1.4737\n",
            "Epoch [9/20], Step [3000/12000], Training Loss: 1.4703\n",
            "Epoch [9/20], Step [3500/12000], Training Loss: 1.4753\n",
            "Epoch [9/20], Step [4000/12000], Training Loss: 1.4703\n",
            "Epoch [9/20], Step [4500/12000], Training Loss: 1.4760\n",
            "Epoch [9/20], Step [5000/12000], Training Loss: 1.4744\n",
            "Epoch [9/20], Step [5500/12000], Training Loss: 1.4731\n",
            "Epoch [9/20], Step [6000/12000], Training Loss: 1.4728\n",
            "Epoch [9/20], Step [6500/12000], Training Loss: 1.4741\n",
            "Epoch [9/20], Step [7000/12000], Training Loss: 1.4767\n",
            "Epoch [9/20], Step [7500/12000], Training Loss: 1.4744\n",
            "Epoch [9/20], Step [8000/12000], Training Loss: 1.4736\n",
            "Epoch [9/20], Step [8500/12000], Training Loss: 1.4741\n",
            "Epoch [9/20], Step [9000/12000], Training Loss: 1.4757\n",
            "Epoch [9/20], Step [9500/12000], Training Loss: 1.4768\n",
            "Epoch [9/20], Step [10000/12000], Training Loss: 1.4752\n",
            "Epoch [9/20], Step [10500/12000], Training Loss: 1.4709\n",
            "Epoch [9/20], Step [11000/12000], Training Loss: 1.4745\n",
            "Epoch [9/20], Step [11500/12000], Training Loss: 1.4789\n",
            "Epoch [9/20], Step [12000/12000], Training Loss: 1.4688\n",
            "Epoch [10/20], Step [500/12000], Training Loss: 1.4715\n",
            "Epoch [10/20], Step [1000/12000], Training Loss: 1.4749\n",
            "Epoch [10/20], Step [1500/12000], Training Loss: 1.4733\n",
            "Epoch [10/20], Step [2000/12000], Training Loss: 1.4746\n",
            "Epoch [10/20], Step [2500/12000], Training Loss: 1.4756\n",
            "Epoch [10/20], Step [3000/12000], Training Loss: 1.4754\n",
            "Epoch [10/20], Step [3500/12000], Training Loss: 1.4716\n",
            "Epoch [10/20], Step [4000/12000], Training Loss: 1.4688\n",
            "Epoch [10/20], Step [4500/12000], Training Loss: 1.4720\n",
            "Epoch [10/20], Step [5000/12000], Training Loss: 1.4751\n",
            "Epoch [10/20], Step [5500/12000], Training Loss: 1.4741\n",
            "Epoch [10/20], Step [6000/12000], Training Loss: 1.4772\n",
            "Epoch [10/20], Step [6500/12000], Training Loss: 1.4699\n",
            "Epoch [10/20], Step [7000/12000], Training Loss: 1.4756\n",
            "Epoch [10/20], Step [7500/12000], Training Loss: 1.4725\n",
            "Epoch [10/20], Step [8000/12000], Training Loss: 1.4698\n",
            "Epoch [10/20], Step [8500/12000], Training Loss: 1.4677\n",
            "Epoch [10/20], Step [9000/12000], Training Loss: 1.4709\n",
            "Epoch [10/20], Step [9500/12000], Training Loss: 1.4720\n",
            "Epoch [10/20], Step [10000/12000], Training Loss: 1.4731\n",
            "Epoch [10/20], Step [10500/12000], Training Loss: 1.4718\n",
            "Epoch [10/20], Step [11000/12000], Training Loss: 1.4708\n",
            "Epoch [10/20], Step [11500/12000], Training Loss: 1.4713\n",
            "Epoch [10/20], Step [12000/12000], Training Loss: 1.4747\n",
            "Epoch [11/20], Step [500/12000], Training Loss: 1.4730\n",
            "Epoch [11/20], Step [1000/12000], Training Loss: 1.4680\n",
            "Epoch [11/20], Step [1500/12000], Training Loss: 1.4721\n",
            "Epoch [11/20], Step [2000/12000], Training Loss: 1.4695\n",
            "Epoch [11/20], Step [2500/12000], Training Loss: 1.4710\n",
            "Epoch [11/20], Step [3000/12000], Training Loss: 1.4699\n",
            "Epoch [11/20], Step [3500/12000], Training Loss: 1.4739\n",
            "Epoch [11/20], Step [4000/12000], Training Loss: 1.4728\n",
            "Epoch [11/20], Step [4500/12000], Training Loss: 1.4684\n",
            "Epoch [11/20], Step [5000/12000], Training Loss: 1.4775\n",
            "Epoch [11/20], Step [5500/12000], Training Loss: 1.4710\n",
            "Epoch [11/20], Step [6000/12000], Training Loss: 1.4703\n",
            "Epoch [11/20], Step [6500/12000], Training Loss: 1.4703\n",
            "Epoch [11/20], Step [7000/12000], Training Loss: 1.4667\n",
            "Epoch [11/20], Step [7500/12000], Training Loss: 1.4725\n",
            "Epoch [11/20], Step [8000/12000], Training Loss: 1.4760\n",
            "Epoch [11/20], Step [8500/12000], Training Loss: 1.4739\n",
            "Epoch [11/20], Step [9000/12000], Training Loss: 1.4706\n"
          ]
        }
      ],
      "source": [
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        images, digits = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer2.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model2(images)\n",
        "        loss = criterion(outputs, digits)\n",
        "        loss.backward()\n",
        "        optimizer2.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 500 == 499:    # print every 500 mini-batches\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Training Loss: {:.4f}' \n",
        "                .format(epoch+1, num_epochs, i+1, total_step, running_loss/500))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIlyCbEXtgDM"
      },
      "source": [
        "CNN3 - Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "QIK-3Morti2z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3b1354b-ee6f-4f05-bc40-e06fbb333444"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Step [500/12000], Training Loss: 2.2924\n",
            "Epoch [1/20], Step [1000/12000], Training Loss: 2.1991\n",
            "Epoch [1/20], Step [1500/12000], Training Loss: 1.8775\n",
            "Epoch [1/20], Step [2000/12000], Training Loss: 1.6981\n",
            "Epoch [1/20], Step [2500/12000], Training Loss: 1.6119\n",
            "Epoch [1/20], Step [3000/12000], Training Loss: 1.5660\n",
            "Epoch [1/20], Step [3500/12000], Training Loss: 1.5484\n",
            "Epoch [1/20], Step [4000/12000], Training Loss: 1.5384\n",
            "Epoch [1/20], Step [4500/12000], Training Loss: 1.5318\n",
            "Epoch [1/20], Step [5000/12000], Training Loss: 1.5299\n",
            "Epoch [1/20], Step [5500/12000], Training Loss: 1.5225\n",
            "Epoch [1/20], Step [6000/12000], Training Loss: 1.5200\n",
            "Epoch [1/20], Step [6500/12000], Training Loss: 1.5188\n",
            "Epoch [1/20], Step [7000/12000], Training Loss: 1.5245\n",
            "Epoch [1/20], Step [7500/12000], Training Loss: 1.5054\n",
            "Epoch [1/20], Step [8000/12000], Training Loss: 1.5081\n",
            "Epoch [1/20], Step [8500/12000], Training Loss: 1.5050\n",
            "Epoch [1/20], Step [9000/12000], Training Loss: 1.5102\n",
            "Epoch [1/20], Step [9500/12000], Training Loss: 1.5093\n",
            "Epoch [1/20], Step [10000/12000], Training Loss: 1.5117\n",
            "Epoch [1/20], Step [10500/12000], Training Loss: 1.5068\n",
            "Epoch [1/20], Step [11000/12000], Training Loss: 1.5034\n",
            "Epoch [1/20], Step [11500/12000], Training Loss: 1.5027\n",
            "Epoch [1/20], Step [12000/12000], Training Loss: 1.4984\n",
            "Epoch [2/20], Step [500/12000], Training Loss: 1.5026\n",
            "Epoch [2/20], Step [1000/12000], Training Loss: 1.5053\n",
            "Epoch [2/20], Step [1500/12000], Training Loss: 1.4941\n",
            "Epoch [2/20], Step [2000/12000], Training Loss: 1.4960\n",
            "Epoch [2/20], Step [2500/12000], Training Loss: 1.5010\n",
            "Epoch [2/20], Step [3000/12000], Training Loss: 1.4929\n",
            "Epoch [2/20], Step [3500/12000], Training Loss: 1.4962\n",
            "Epoch [2/20], Step [4000/12000], Training Loss: 1.4983\n",
            "Epoch [2/20], Step [4500/12000], Training Loss: 1.5015\n",
            "Epoch [2/20], Step [5000/12000], Training Loss: 1.5034\n",
            "Epoch [2/20], Step [5500/12000], Training Loss: 1.4919\n",
            "Epoch [2/20], Step [6000/12000], Training Loss: 1.4985\n",
            "Epoch [2/20], Step [6500/12000], Training Loss: 1.4906\n",
            "Epoch [2/20], Step [7000/12000], Training Loss: 1.4918\n",
            "Epoch [2/20], Step [7500/12000], Training Loss: 1.4911\n",
            "Epoch [2/20], Step [8000/12000], Training Loss: 1.4894\n",
            "Epoch [2/20], Step [8500/12000], Training Loss: 1.4933\n",
            "Epoch [2/20], Step [9000/12000], Training Loss: 1.4905\n",
            "Epoch [2/20], Step [9500/12000], Training Loss: 1.4974\n",
            "Epoch [2/20], Step [10000/12000], Training Loss: 1.4961\n",
            "Epoch [2/20], Step [10500/12000], Training Loss: 1.4891\n",
            "Epoch [2/20], Step [11000/12000], Training Loss: 1.4942\n",
            "Epoch [2/20], Step [11500/12000], Training Loss: 1.4941\n",
            "Epoch [2/20], Step [12000/12000], Training Loss: 1.4863\n",
            "Epoch [3/20], Step [500/12000], Training Loss: 1.4904\n",
            "Epoch [3/20], Step [1000/12000], Training Loss: 1.4879\n",
            "Epoch [3/20], Step [1500/12000], Training Loss: 1.4873\n",
            "Epoch [3/20], Step [2000/12000], Training Loss: 1.4860\n",
            "Epoch [3/20], Step [2500/12000], Training Loss: 1.4907\n",
            "Epoch [3/20], Step [3000/12000], Training Loss: 1.4879\n",
            "Epoch [3/20], Step [3500/12000], Training Loss: 1.4897\n",
            "Epoch [3/20], Step [4000/12000], Training Loss: 1.4890\n",
            "Epoch [3/20], Step [4500/12000], Training Loss: 1.4895\n",
            "Epoch [3/20], Step [5000/12000], Training Loss: 1.4904\n",
            "Epoch [3/20], Step [5500/12000], Training Loss: 1.4889\n",
            "Epoch [3/20], Step [6000/12000], Training Loss: 1.4848\n",
            "Epoch [3/20], Step [6500/12000], Training Loss: 1.4865\n",
            "Epoch [3/20], Step [7000/12000], Training Loss: 1.4835\n",
            "Epoch [3/20], Step [7500/12000], Training Loss: 1.4889\n",
            "Epoch [3/20], Step [8000/12000], Training Loss: 1.4890\n",
            "Epoch [3/20], Step [8500/12000], Training Loss: 1.4840\n",
            "Epoch [3/20], Step [9000/12000], Training Loss: 1.4849\n",
            "Epoch [3/20], Step [9500/12000], Training Loss: 1.4815\n",
            "Epoch [3/20], Step [10000/12000], Training Loss: 1.4874\n",
            "Epoch [3/20], Step [10500/12000], Training Loss: 1.4876\n",
            "Epoch [3/20], Step [11000/12000], Training Loss: 1.4839\n",
            "Epoch [3/20], Step [11500/12000], Training Loss: 1.4832\n",
            "Epoch [3/20], Step [12000/12000], Training Loss: 1.4833\n",
            "Epoch [4/20], Step [500/12000], Training Loss: 1.4806\n",
            "Epoch [4/20], Step [1000/12000], Training Loss: 1.4881\n",
            "Epoch [4/20], Step [1500/12000], Training Loss: 1.4780\n",
            "Epoch [4/20], Step [2000/12000], Training Loss: 1.4775\n",
            "Epoch [4/20], Step [2500/12000], Training Loss: 1.4836\n",
            "Epoch [4/20], Step [3000/12000], Training Loss: 1.4814\n",
            "Epoch [4/20], Step [3500/12000], Training Loss: 1.4800\n",
            "Epoch [4/20], Step [4000/12000], Training Loss: 1.4842\n",
            "Epoch [4/20], Step [4500/12000], Training Loss: 1.4829\n",
            "Epoch [4/20], Step [5000/12000], Training Loss: 1.4807\n",
            "Epoch [4/20], Step [5500/12000], Training Loss: 1.4840\n",
            "Epoch [4/20], Step [6000/12000], Training Loss: 1.4893\n",
            "Epoch [4/20], Step [6500/12000], Training Loss: 1.4847\n",
            "Epoch [4/20], Step [7000/12000], Training Loss: 1.4810\n",
            "Epoch [4/20], Step [7500/12000], Training Loss: 1.4814\n",
            "Epoch [4/20], Step [8000/12000], Training Loss: 1.4847\n",
            "Epoch [4/20], Step [8500/12000], Training Loss: 1.4848\n",
            "Epoch [4/20], Step [9000/12000], Training Loss: 1.4814\n",
            "Epoch [4/20], Step [9500/12000], Training Loss: 1.4818\n",
            "Epoch [4/20], Step [10000/12000], Training Loss: 1.4804\n",
            "Epoch [4/20], Step [10500/12000], Training Loss: 1.4797\n",
            "Epoch [4/20], Step [11000/12000], Training Loss: 1.4785\n",
            "Epoch [4/20], Step [11500/12000], Training Loss: 1.4808\n",
            "Epoch [4/20], Step [12000/12000], Training Loss: 1.4804\n",
            "Epoch [5/20], Step [500/12000], Training Loss: 1.4815\n",
            "Epoch [5/20], Step [1000/12000], Training Loss: 1.4763\n",
            "Epoch [5/20], Step [1500/12000], Training Loss: 1.4813\n",
            "Epoch [5/20], Step [2000/12000], Training Loss: 1.4791\n",
            "Epoch [5/20], Step [2500/12000], Training Loss: 1.4804\n",
            "Epoch [5/20], Step [3000/12000], Training Loss: 1.4846\n",
            "Epoch [5/20], Step [3500/12000], Training Loss: 1.4839\n",
            "Epoch [5/20], Step [4000/12000], Training Loss: 1.4809\n",
            "Epoch [5/20], Step [4500/12000], Training Loss: 1.4777\n",
            "Epoch [5/20], Step [5000/12000], Training Loss: 1.4836\n",
            "Epoch [5/20], Step [5500/12000], Training Loss: 1.4798\n",
            "Epoch [5/20], Step [6000/12000], Training Loss: 1.4771\n",
            "Epoch [5/20], Step [6500/12000], Training Loss: 1.4773\n",
            "Epoch [5/20], Step [7000/12000], Training Loss: 1.4799\n",
            "Epoch [5/20], Step [7500/12000], Training Loss: 1.4769\n",
            "Epoch [5/20], Step [8000/12000], Training Loss: 1.4795\n",
            "Epoch [5/20], Step [8500/12000], Training Loss: 1.4898\n",
            "Epoch [5/20], Step [9000/12000], Training Loss: 1.4750\n",
            "Epoch [5/20], Step [9500/12000], Training Loss: 1.4784\n",
            "Epoch [5/20], Step [10000/12000], Training Loss: 1.4793\n",
            "Epoch [5/20], Step [10500/12000], Training Loss: 1.4758\n",
            "Epoch [5/20], Step [11000/12000], Training Loss: 1.4742\n",
            "Epoch [5/20], Step [11500/12000], Training Loss: 1.4769\n",
            "Epoch [5/20], Step [12000/12000], Training Loss: 1.4800\n",
            "Epoch [6/20], Step [500/12000], Training Loss: 1.4775\n",
            "Epoch [6/20], Step [1000/12000], Training Loss: 1.4794\n",
            "Epoch [6/20], Step [1500/12000], Training Loss: 1.4809\n",
            "Epoch [6/20], Step [2000/12000], Training Loss: 1.4773\n",
            "Epoch [6/20], Step [2500/12000], Training Loss: 1.4779\n",
            "Epoch [6/20], Step [3000/12000], Training Loss: 1.4763\n",
            "Epoch [6/20], Step [3500/12000], Training Loss: 1.4771\n",
            "Epoch [6/20], Step [4000/12000], Training Loss: 1.4793\n",
            "Epoch [6/20], Step [4500/12000], Training Loss: 1.4738\n",
            "Epoch [6/20], Step [5000/12000], Training Loss: 1.4793\n",
            "Epoch [6/20], Step [5500/12000], Training Loss: 1.4803\n",
            "Epoch [6/20], Step [6000/12000], Training Loss: 1.4764\n",
            "Epoch [6/20], Step [6500/12000], Training Loss: 1.4781\n",
            "Epoch [6/20], Step [7000/12000], Training Loss: 1.4768\n",
            "Epoch [6/20], Step [7500/12000], Training Loss: 1.4779\n",
            "Epoch [6/20], Step [8000/12000], Training Loss: 1.4778\n",
            "Epoch [6/20], Step [8500/12000], Training Loss: 1.4759\n",
            "Epoch [6/20], Step [9000/12000], Training Loss: 1.4767\n",
            "Epoch [6/20], Step [9500/12000], Training Loss: 1.4805\n",
            "Epoch [6/20], Step [10000/12000], Training Loss: 1.4785\n",
            "Epoch [6/20], Step [10500/12000], Training Loss: 1.4714\n",
            "Epoch [6/20], Step [11000/12000], Training Loss: 1.4800\n",
            "Epoch [6/20], Step [11500/12000], Training Loss: 1.4738\n",
            "Epoch [6/20], Step [12000/12000], Training Loss: 1.4717\n",
            "Epoch [7/20], Step [500/12000], Training Loss: 1.4745\n",
            "Epoch [7/20], Step [1000/12000], Training Loss: 1.4778\n",
            "Epoch [7/20], Step [1500/12000], Training Loss: 1.4701\n",
            "Epoch [7/20], Step [2000/12000], Training Loss: 1.4805\n",
            "Epoch [7/20], Step [2500/12000], Training Loss: 1.4779\n",
            "Epoch [7/20], Step [3000/12000], Training Loss: 1.4750\n",
            "Epoch [7/20], Step [3500/12000], Training Loss: 1.4739\n",
            "Epoch [7/20], Step [4000/12000], Training Loss: 1.4772\n",
            "Epoch [7/20], Step [4500/12000], Training Loss: 1.4790\n",
            "Epoch [7/20], Step [5000/12000], Training Loss: 1.4788\n",
            "Epoch [7/20], Step [5500/12000], Training Loss: 1.4749\n",
            "Epoch [7/20], Step [6000/12000], Training Loss: 1.4686\n",
            "Epoch [7/20], Step [6500/12000], Training Loss: 1.4749\n",
            "Epoch [7/20], Step [7000/12000], Training Loss: 1.4758\n",
            "Epoch [7/20], Step [7500/12000], Training Loss: 1.4786\n",
            "Epoch [7/20], Step [8000/12000], Training Loss: 1.4719\n",
            "Epoch [7/20], Step [8500/12000], Training Loss: 1.4793\n",
            "Epoch [7/20], Step [9000/12000], Training Loss: 1.4748\n",
            "Epoch [7/20], Step [9500/12000], Training Loss: 1.4736\n",
            "Epoch [7/20], Step [10000/12000], Training Loss: 1.4781\n",
            "Epoch [7/20], Step [10500/12000], Training Loss: 1.4710\n",
            "Epoch [7/20], Step [11000/12000], Training Loss: 1.4713\n",
            "Epoch [7/20], Step [11500/12000], Training Loss: 1.4737\n",
            "Epoch [7/20], Step [12000/12000], Training Loss: 1.4751\n",
            "Epoch [8/20], Step [500/12000], Training Loss: 1.4747\n",
            "Epoch [8/20], Step [1000/12000], Training Loss: 1.4778\n",
            "Epoch [8/20], Step [1500/12000], Training Loss: 1.4753\n",
            "Epoch [8/20], Step [2000/12000], Training Loss: 1.4744\n",
            "Epoch [8/20], Step [2500/12000], Training Loss: 1.4727\n",
            "Epoch [8/20], Step [3000/12000], Training Loss: 1.4735\n",
            "Epoch [8/20], Step [3500/12000], Training Loss: 1.4755\n",
            "Epoch [8/20], Step [4000/12000], Training Loss: 1.4723\n",
            "Epoch [8/20], Step [4500/12000], Training Loss: 1.4705\n",
            "Epoch [8/20], Step [5000/12000], Training Loss: 1.4721\n",
            "Epoch [8/20], Step [5500/12000], Training Loss: 1.4755\n",
            "Epoch [8/20], Step [6000/12000], Training Loss: 1.4749\n",
            "Epoch [8/20], Step [6500/12000], Training Loss: 1.4732\n",
            "Epoch [8/20], Step [7000/12000], Training Loss: 1.4711\n",
            "Epoch [8/20], Step [7500/12000], Training Loss: 1.4772\n",
            "Epoch [8/20], Step [8000/12000], Training Loss: 1.4726\n",
            "Epoch [8/20], Step [8500/12000], Training Loss: 1.4773\n",
            "Epoch [8/20], Step [9000/12000], Training Loss: 1.4733\n",
            "Epoch [8/20], Step [9500/12000], Training Loss: 1.4762\n",
            "Epoch [8/20], Step [10000/12000], Training Loss: 1.4737\n",
            "Epoch [8/20], Step [10500/12000], Training Loss: 1.4717\n",
            "Epoch [8/20], Step [11000/12000], Training Loss: 1.4767\n",
            "Epoch [8/20], Step [11500/12000], Training Loss: 1.4741\n",
            "Epoch [8/20], Step [12000/12000], Training Loss: 1.4787\n",
            "Epoch [9/20], Step [500/12000], Training Loss: 1.4700\n",
            "Epoch [9/20], Step [1000/12000], Training Loss: 1.4724\n",
            "Epoch [9/20], Step [1500/12000], Training Loss: 1.4700\n",
            "Epoch [9/20], Step [2000/12000], Training Loss: 1.4727\n",
            "Epoch [9/20], Step [2500/12000], Training Loss: 1.4745\n",
            "Epoch [9/20], Step [3000/12000], Training Loss: 1.4722\n",
            "Epoch [9/20], Step [3500/12000], Training Loss: 1.4717\n",
            "Epoch [9/20], Step [4000/12000], Training Loss: 1.4709\n",
            "Epoch [9/20], Step [4500/12000], Training Loss: 1.4726\n",
            "Epoch [9/20], Step [5000/12000], Training Loss: 1.4732\n",
            "Epoch [9/20], Step [5500/12000], Training Loss: 1.4719\n",
            "Epoch [9/20], Step [6000/12000], Training Loss: 1.4744\n",
            "Epoch [9/20], Step [6500/12000], Training Loss: 1.4723\n",
            "Epoch [9/20], Step [7000/12000], Training Loss: 1.4728\n",
            "Epoch [9/20], Step [7500/12000], Training Loss: 1.4747\n",
            "Epoch [9/20], Step [8000/12000], Training Loss: 1.4751\n",
            "Epoch [9/20], Step [8500/12000], Training Loss: 1.4712\n",
            "Epoch [9/20], Step [9000/12000], Training Loss: 1.4697\n",
            "Epoch [9/20], Step [9500/12000], Training Loss: 1.4708\n",
            "Epoch [9/20], Step [10000/12000], Training Loss: 1.4725\n",
            "Epoch [9/20], Step [10500/12000], Training Loss: 1.4755\n",
            "Epoch [9/20], Step [11000/12000], Training Loss: 1.4740\n",
            "Epoch [9/20], Step [11500/12000], Training Loss: 1.4733\n",
            "Epoch [9/20], Step [12000/12000], Training Loss: 1.4775\n",
            "Epoch [10/20], Step [500/12000], Training Loss: 1.4743\n",
            "Epoch [10/20], Step [1000/12000], Training Loss: 1.4700\n",
            "Epoch [10/20], Step [1500/12000], Training Loss: 1.4747\n",
            "Epoch [10/20], Step [2000/12000], Training Loss: 1.4718\n",
            "Epoch [10/20], Step [2500/12000], Training Loss: 1.4747\n",
            "Epoch [10/20], Step [3000/12000], Training Loss: 1.4699\n",
            "Epoch [10/20], Step [3500/12000], Training Loss: 1.4739\n",
            "Epoch [10/20], Step [4000/12000], Training Loss: 1.4723\n",
            "Epoch [10/20], Step [4500/12000], Training Loss: 1.4739\n",
            "Epoch [10/20], Step [5000/12000], Training Loss: 1.4736\n",
            "Epoch [10/20], Step [5500/12000], Training Loss: 1.4692\n",
            "Epoch [10/20], Step [6000/12000], Training Loss: 1.4705\n",
            "Epoch [10/20], Step [6500/12000], Training Loss: 1.4692\n",
            "Epoch [10/20], Step [7000/12000], Training Loss: 1.4714\n",
            "Epoch [10/20], Step [7500/12000], Training Loss: 1.4697\n",
            "Epoch [10/20], Step [8000/12000], Training Loss: 1.4716\n",
            "Epoch [10/20], Step [8500/12000], Training Loss: 1.4717\n",
            "Epoch [10/20], Step [9000/12000], Training Loss: 1.4711\n",
            "Epoch [10/20], Step [9500/12000], Training Loss: 1.4743\n",
            "Epoch [10/20], Step [10000/12000], Training Loss: 1.4712\n",
            "Epoch [10/20], Step [10500/12000], Training Loss: 1.4725\n",
            "Epoch [10/20], Step [11000/12000], Training Loss: 1.4716\n",
            "Epoch [10/20], Step [11500/12000], Training Loss: 1.4728\n",
            "Epoch [10/20], Step [12000/12000], Training Loss: 1.4735\n",
            "Epoch [11/20], Step [500/12000], Training Loss: 1.4702\n",
            "Epoch [11/20], Step [1000/12000], Training Loss: 1.4686\n",
            "Epoch [11/20], Step [1500/12000], Training Loss: 1.4738\n",
            "Epoch [11/20], Step [2000/12000], Training Loss: 1.4738\n",
            "Epoch [11/20], Step [2500/12000], Training Loss: 1.4748\n",
            "Epoch [11/20], Step [3000/12000], Training Loss: 1.4681\n",
            "Epoch [11/20], Step [3500/12000], Training Loss: 1.4704\n",
            "Epoch [11/20], Step [4000/12000], Training Loss: 1.4718\n",
            "Epoch [11/20], Step [4500/12000], Training Loss: 1.4711\n",
            "Epoch [11/20], Step [5000/12000], Training Loss: 1.4717\n",
            "Epoch [11/20], Step [5500/12000], Training Loss: 1.4734\n",
            "Epoch [11/20], Step [6000/12000], Training Loss: 1.4745\n",
            "Epoch [11/20], Step [6500/12000], Training Loss: 1.4744\n",
            "Epoch [11/20], Step [7000/12000], Training Loss: 1.4719\n",
            "Epoch [11/20], Step [7500/12000], Training Loss: 1.4725\n",
            "Epoch [11/20], Step [8000/12000], Training Loss: 1.4707\n",
            "Epoch [11/20], Step [8500/12000], Training Loss: 1.4705\n",
            "Epoch [11/20], Step [9000/12000], Training Loss: 1.4712\n",
            "Epoch [11/20], Step [9500/12000], Training Loss: 1.4709\n",
            "Epoch [11/20], Step [10000/12000], Training Loss: 1.4704\n",
            "Epoch [11/20], Step [10500/12000], Training Loss: 1.4705\n",
            "Epoch [11/20], Step [11000/12000], Training Loss: 1.4703\n",
            "Epoch [11/20], Step [11500/12000], Training Loss: 1.4704\n",
            "Epoch [11/20], Step [12000/12000], Training Loss: 1.4691\n",
            "Epoch [12/20], Step [500/12000], Training Loss: 1.4709\n",
            "Epoch [12/20], Step [1000/12000], Training Loss: 1.4680\n",
            "Epoch [12/20], Step [1500/12000], Training Loss: 1.4681\n",
            "Epoch [12/20], Step [2000/12000], Training Loss: 1.4708\n",
            "Epoch [12/20], Step [2500/12000], Training Loss: 1.4685\n",
            "Epoch [12/20], Step [3000/12000], Training Loss: 1.4729\n",
            "Epoch [12/20], Step [3500/12000], Training Loss: 1.4745\n",
            "Epoch [12/20], Step [4000/12000], Training Loss: 1.4708\n",
            "Epoch [12/20], Step [4500/12000], Training Loss: 1.4713\n",
            "Epoch [12/20], Step [5000/12000], Training Loss: 1.4726\n",
            "Epoch [12/20], Step [5500/12000], Training Loss: 1.4706\n",
            "Epoch [12/20], Step [6000/12000], Training Loss: 1.4704\n",
            "Epoch [12/20], Step [6500/12000], Training Loss: 1.4687\n",
            "Epoch [12/20], Step [7000/12000], Training Loss: 1.4712\n",
            "Epoch [12/20], Step [7500/12000], Training Loss: 1.4692\n",
            "Epoch [12/20], Step [8000/12000], Training Loss: 1.4719\n",
            "Epoch [12/20], Step [8500/12000], Training Loss: 1.4685\n",
            "Epoch [12/20], Step [9000/12000], Training Loss: 1.4674\n",
            "Epoch [12/20], Step [9500/12000], Training Loss: 1.4757\n",
            "Epoch [12/20], Step [10000/12000], Training Loss: 1.4708\n",
            "Epoch [12/20], Step [10500/12000], Training Loss: 1.4688\n",
            "Epoch [12/20], Step [11000/12000], Training Loss: 1.4694\n",
            "Epoch [12/20], Step [11500/12000], Training Loss: 1.4730\n",
            "Epoch [12/20], Step [12000/12000], Training Loss: 1.4689\n",
            "Epoch [13/20], Step [500/12000], Training Loss: 1.4674\n",
            "Epoch [13/20], Step [1000/12000], Training Loss: 1.4684\n",
            "Epoch [13/20], Step [1500/12000], Training Loss: 1.4680\n",
            "Epoch [13/20], Step [2000/12000], Training Loss: 1.4672\n",
            "Epoch [13/20], Step [2500/12000], Training Loss: 1.4704\n",
            "Epoch [13/20], Step [3000/12000], Training Loss: 1.4739\n",
            "Epoch [13/20], Step [3500/12000], Training Loss: 1.4704\n",
            "Epoch [13/20], Step [4000/12000], Training Loss: 1.4657\n",
            "Epoch [13/20], Step [4500/12000], Training Loss: 1.4688\n",
            "Epoch [13/20], Step [5000/12000], Training Loss: 1.4692\n",
            "Epoch [13/20], Step [5500/12000], Training Loss: 1.4725\n",
            "Epoch [13/20], Step [6000/12000], Training Loss: 1.4709\n",
            "Epoch [13/20], Step [6500/12000], Training Loss: 1.4678\n",
            "Epoch [13/20], Step [7000/12000], Training Loss: 1.4729\n",
            "Epoch [13/20], Step [7500/12000], Training Loss: 1.4719\n",
            "Epoch [13/20], Step [8000/12000], Training Loss: 1.4685\n",
            "Epoch [13/20], Step [8500/12000], Training Loss: 1.4739\n",
            "Epoch [13/20], Step [9000/12000], Training Loss: 1.4739\n",
            "Epoch [13/20], Step [9500/12000], Training Loss: 1.4673\n",
            "Epoch [13/20], Step [10000/12000], Training Loss: 1.4723\n",
            "Epoch [13/20], Step [10500/12000], Training Loss: 1.4726\n",
            "Epoch [13/20], Step [11000/12000], Training Loss: 1.4701\n",
            "Epoch [13/20], Step [11500/12000], Training Loss: 1.4722\n",
            "Epoch [13/20], Step [12000/12000], Training Loss: 1.4678\n",
            "Epoch [14/20], Step [500/12000], Training Loss: 1.4693\n",
            "Epoch [14/20], Step [1000/12000], Training Loss: 1.4677\n",
            "Epoch [14/20], Step [1500/12000], Training Loss: 1.4645\n",
            "Epoch [14/20], Step [2000/12000], Training Loss: 1.4689\n",
            "Epoch [14/20], Step [2500/12000], Training Loss: 1.4689\n",
            "Epoch [14/20], Step [3000/12000], Training Loss: 1.4668\n",
            "Epoch [14/20], Step [3500/12000], Training Loss: 1.4706\n",
            "Epoch [14/20], Step [4000/12000], Training Loss: 1.4688\n",
            "Epoch [14/20], Step [4500/12000], Training Loss: 1.4707\n",
            "Epoch [14/20], Step [5000/12000], Training Loss: 1.4686\n",
            "Epoch [14/20], Step [5500/12000], Training Loss: 1.4679\n",
            "Epoch [14/20], Step [6000/12000], Training Loss: 1.4684\n",
            "Epoch [14/20], Step [6500/12000], Training Loss: 1.4705\n",
            "Epoch [14/20], Step [7000/12000], Training Loss: 1.4708\n",
            "Epoch [14/20], Step [7500/12000], Training Loss: 1.4719\n",
            "Epoch [14/20], Step [8000/12000], Training Loss: 1.4720\n",
            "Epoch [14/20], Step [8500/12000], Training Loss: 1.4706\n",
            "Epoch [14/20], Step [9000/12000], Training Loss: 1.4713\n",
            "Epoch [14/20], Step [9500/12000], Training Loss: 1.4695\n",
            "Epoch [14/20], Step [10000/12000], Training Loss: 1.4663\n",
            "Epoch [14/20], Step [10500/12000], Training Loss: 1.4698\n",
            "Epoch [14/20], Step [11000/12000], Training Loss: 1.4693\n",
            "Epoch [14/20], Step [11500/12000], Training Loss: 1.4705\n",
            "Epoch [14/20], Step [12000/12000], Training Loss: 1.4693\n",
            "Epoch [15/20], Step [500/12000], Training Loss: 1.4686\n",
            "Epoch [15/20], Step [1000/12000], Training Loss: 1.4680\n",
            "Epoch [15/20], Step [1500/12000], Training Loss: 1.4680\n",
            "Epoch [15/20], Step [2000/12000], Training Loss: 1.4705\n",
            "Epoch [15/20], Step [2500/12000], Training Loss: 1.4706\n",
            "Epoch [15/20], Step [3000/12000], Training Loss: 1.4667\n",
            "Epoch [15/20], Step [3500/12000], Training Loss: 1.4671\n",
            "Epoch [15/20], Step [4000/12000], Training Loss: 1.4697\n",
            "Epoch [15/20], Step [4500/12000], Training Loss: 1.4656\n",
            "Epoch [15/20], Step [5000/12000], Training Loss: 1.4693\n",
            "Epoch [15/20], Step [5500/12000], Training Loss: 1.4690\n",
            "Epoch [15/20], Step [6000/12000], Training Loss: 1.4683\n",
            "Epoch [15/20], Step [6500/12000], Training Loss: 1.4673\n",
            "Epoch [15/20], Step [7000/12000], Training Loss: 1.4682\n",
            "Epoch [15/20], Step [7500/12000], Training Loss: 1.4688\n",
            "Epoch [15/20], Step [8000/12000], Training Loss: 1.4682\n",
            "Epoch [15/20], Step [8500/12000], Training Loss: 1.4685\n",
            "Epoch [15/20], Step [9000/12000], Training Loss: 1.4656\n",
            "Epoch [15/20], Step [9500/12000], Training Loss: 1.4716\n",
            "Epoch [15/20], Step [10000/12000], Training Loss: 1.4671\n",
            "Epoch [15/20], Step [10500/12000], Training Loss: 1.4698\n",
            "Epoch [15/20], Step [11000/12000], Training Loss: 1.4669\n",
            "Epoch [15/20], Step [11500/12000], Training Loss: 1.4685\n",
            "Epoch [15/20], Step [12000/12000], Training Loss: 1.4688\n",
            "Epoch [16/20], Step [500/12000], Training Loss: 1.4651\n",
            "Epoch [16/20], Step [1000/12000], Training Loss: 1.4635\n",
            "Epoch [16/20], Step [1500/12000], Training Loss: 1.4659\n",
            "Epoch [16/20], Step [2000/12000], Training Loss: 1.4692\n",
            "Epoch [16/20], Step [2500/12000], Training Loss: 1.4687\n",
            "Epoch [16/20], Step [3000/12000], Training Loss: 1.4678\n",
            "Epoch [16/20], Step [3500/12000], Training Loss: 1.4709\n",
            "Epoch [16/20], Step [4000/12000], Training Loss: 1.4691\n",
            "Epoch [16/20], Step [4500/12000], Training Loss: 1.4698\n",
            "Epoch [16/20], Step [5000/12000], Training Loss: 1.4686\n",
            "Epoch [16/20], Step [5500/12000], Training Loss: 1.4708\n",
            "Epoch [16/20], Step [6000/12000], Training Loss: 1.4684\n",
            "Epoch [16/20], Step [6500/12000], Training Loss: 1.4664\n",
            "Epoch [16/20], Step [7000/12000], Training Loss: 1.4680\n",
            "Epoch [16/20], Step [7500/12000], Training Loss: 1.4689\n",
            "Epoch [16/20], Step [8000/12000], Training Loss: 1.4688\n",
            "Epoch [16/20], Step [8500/12000], Training Loss: 1.4709\n",
            "Epoch [16/20], Step [9000/12000], Training Loss: 1.4708\n",
            "Epoch [16/20], Step [9500/12000], Training Loss: 1.4661\n",
            "Epoch [16/20], Step [10000/12000], Training Loss: 1.4693\n",
            "Epoch [16/20], Step [10500/12000], Training Loss: 1.4697\n",
            "Epoch [16/20], Step [11000/12000], Training Loss: 1.4683\n",
            "Epoch [16/20], Step [11500/12000], Training Loss: 1.4717\n",
            "Epoch [16/20], Step [12000/12000], Training Loss: 1.4683\n",
            "Epoch [17/20], Step [500/12000], Training Loss: 1.4676\n",
            "Epoch [17/20], Step [1000/12000], Training Loss: 1.4679\n",
            "Epoch [17/20], Step [1500/12000], Training Loss: 1.4678\n",
            "Epoch [17/20], Step [2000/12000], Training Loss: 1.4684\n",
            "Epoch [17/20], Step [2500/12000], Training Loss: 1.4666\n",
            "Epoch [17/20], Step [3000/12000], Training Loss: 1.4655\n",
            "Epoch [17/20], Step [3500/12000], Training Loss: 1.4678\n",
            "Epoch [17/20], Step [4000/12000], Training Loss: 1.4678\n",
            "Epoch [17/20], Step [4500/12000], Training Loss: 1.4678\n",
            "Epoch [17/20], Step [5000/12000], Training Loss: 1.4699\n",
            "Epoch [17/20], Step [5500/12000], Training Loss: 1.4683\n",
            "Epoch [17/20], Step [6000/12000], Training Loss: 1.4686\n",
            "Epoch [17/20], Step [6500/12000], Training Loss: 1.4675\n",
            "Epoch [17/20], Step [7000/12000], Training Loss: 1.4704\n",
            "Epoch [17/20], Step [7500/12000], Training Loss: 1.4664\n",
            "Epoch [17/20], Step [8000/12000], Training Loss: 1.4672\n",
            "Epoch [17/20], Step [8500/12000], Training Loss: 1.4654\n",
            "Epoch [17/20], Step [9000/12000], Training Loss: 1.4672\n",
            "Epoch [17/20], Step [9500/12000], Training Loss: 1.4666\n",
            "Epoch [17/20], Step [10000/12000], Training Loss: 1.4687\n",
            "Epoch [17/20], Step [10500/12000], Training Loss: 1.4690\n",
            "Epoch [17/20], Step [11000/12000], Training Loss: 1.4730\n",
            "Epoch [17/20], Step [11500/12000], Training Loss: 1.4683\n",
            "Epoch [17/20], Step [12000/12000], Training Loss: 1.4701\n",
            "Epoch [18/20], Step [500/12000], Training Loss: 1.4715\n",
            "Epoch [18/20], Step [1000/12000], Training Loss: 1.4675\n",
            "Epoch [18/20], Step [1500/12000], Training Loss: 1.4691\n",
            "Epoch [18/20], Step [2000/12000], Training Loss: 1.4639\n",
            "Epoch [18/20], Step [2500/12000], Training Loss: 1.4666\n",
            "Epoch [18/20], Step [3000/12000], Training Loss: 1.4685\n",
            "Epoch [18/20], Step [3500/12000], Training Loss: 1.4651\n",
            "Epoch [18/20], Step [4000/12000], Training Loss: 1.4684\n",
            "Epoch [18/20], Step [4500/12000], Training Loss: 1.4691\n",
            "Epoch [18/20], Step [5000/12000], Training Loss: 1.4694\n",
            "Epoch [18/20], Step [5500/12000], Training Loss: 1.4690\n",
            "Epoch [18/20], Step [6000/12000], Training Loss: 1.4661\n",
            "Epoch [18/20], Step [6500/12000], Training Loss: 1.4676\n",
            "Epoch [18/20], Step [7000/12000], Training Loss: 1.4657\n",
            "Epoch [18/20], Step [7500/12000], Training Loss: 1.4650\n",
            "Epoch [18/20], Step [8000/12000], Training Loss: 1.4693\n",
            "Epoch [18/20], Step [8500/12000], Training Loss: 1.4664\n",
            "Epoch [18/20], Step [9000/12000], Training Loss: 1.4667\n",
            "Epoch [18/20], Step [9500/12000], Training Loss: 1.4669\n",
            "Epoch [18/20], Step [10000/12000], Training Loss: 1.4706\n",
            "Epoch [18/20], Step [10500/12000], Training Loss: 1.4637\n",
            "Epoch [18/20], Step [11000/12000], Training Loss: 1.4683\n",
            "Epoch [18/20], Step [11500/12000], Training Loss: 1.4690\n",
            "Epoch [18/20], Step [12000/12000], Training Loss: 1.4679\n",
            "Epoch [19/20], Step [500/12000], Training Loss: 1.4674\n",
            "Epoch [19/20], Step [1000/12000], Training Loss: 1.4684\n",
            "Epoch [19/20], Step [1500/12000], Training Loss: 1.4637\n",
            "Epoch [19/20], Step [2000/12000], Training Loss: 1.4680\n",
            "Epoch [19/20], Step [2500/12000], Training Loss: 1.4649\n",
            "Epoch [19/20], Step [3000/12000], Training Loss: 1.4684\n",
            "Epoch [19/20], Step [3500/12000], Training Loss: 1.4667\n",
            "Epoch [19/20], Step [4000/12000], Training Loss: 1.4659\n",
            "Epoch [19/20], Step [4500/12000], Training Loss: 1.4652\n",
            "Epoch [19/20], Step [5000/12000], Training Loss: 1.4672\n",
            "Epoch [19/20], Step [5500/12000], Training Loss: 1.4674\n",
            "Epoch [19/20], Step [6000/12000], Training Loss: 1.4655\n",
            "Epoch [19/20], Step [6500/12000], Training Loss: 1.4672\n",
            "Epoch [19/20], Step [7000/12000], Training Loss: 1.4708\n",
            "Epoch [19/20], Step [7500/12000], Training Loss: 1.4662\n",
            "Epoch [19/20], Step [8000/12000], Training Loss: 1.4670\n",
            "Epoch [19/20], Step [8500/12000], Training Loss: 1.4670\n",
            "Epoch [19/20], Step [9000/12000], Training Loss: 1.4675\n",
            "Epoch [19/20], Step [9500/12000], Training Loss: 1.4678\n",
            "Epoch [19/20], Step [10000/12000], Training Loss: 1.4671\n",
            "Epoch [19/20], Step [10500/12000], Training Loss: 1.4644\n",
            "Epoch [19/20], Step [11000/12000], Training Loss: 1.4686\n",
            "Epoch [19/20], Step [11500/12000], Training Loss: 1.4680\n",
            "Epoch [19/20], Step [12000/12000], Training Loss: 1.4661\n",
            "Epoch [20/20], Step [500/12000], Training Loss: 1.4639\n",
            "Epoch [20/20], Step [1000/12000], Training Loss: 1.4662\n",
            "Epoch [20/20], Step [1500/12000], Training Loss: 1.4694\n",
            "Epoch [20/20], Step [2000/12000], Training Loss: 1.4674\n",
            "Epoch [20/20], Step [2500/12000], Training Loss: 1.4687\n",
            "Epoch [20/20], Step [3000/12000], Training Loss: 1.4667\n",
            "Epoch [20/20], Step [3500/12000], Training Loss: 1.4681\n",
            "Epoch [20/20], Step [4000/12000], Training Loss: 1.4645\n",
            "Epoch [20/20], Step [4500/12000], Training Loss: 1.4669\n",
            "Epoch [20/20], Step [5000/12000], Training Loss: 1.4691\n",
            "Epoch [20/20], Step [5500/12000], Training Loss: 1.4667\n",
            "Epoch [20/20], Step [6000/12000], Training Loss: 1.4671\n",
            "Epoch [20/20], Step [6500/12000], Training Loss: 1.4668\n",
            "Epoch [20/20], Step [7000/12000], Training Loss: 1.4663\n",
            "Epoch [20/20], Step [7500/12000], Training Loss: 1.4661\n",
            "Epoch [20/20], Step [8000/12000], Training Loss: 1.4704\n",
            "Epoch [20/20], Step [8500/12000], Training Loss: 1.4656\n",
            "Epoch [20/20], Step [9000/12000], Training Loss: 1.4650\n",
            "Epoch [20/20], Step [9500/12000], Training Loss: 1.4693\n",
            "Epoch [20/20], Step [10000/12000], Training Loss: 1.4636\n",
            "Epoch [20/20], Step [10500/12000], Training Loss: 1.4662\n",
            "Epoch [20/20], Step [11000/12000], Training Loss: 1.4699\n",
            "Epoch [20/20], Step [11500/12000], Training Loss: 1.4661\n",
            "Epoch [20/20], Step [12000/12000], Training Loss: 1.4691\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        images, digits = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer3.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model3(images)\n",
        "        loss = criterion(outputs, digits)\n",
        "        loss.backward()\n",
        "        optimizer3.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 500 == 499:    # print every 500 mini-batches\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Training Loss: {:.4f}' \n",
        "                .format(epoch+1, num_epochs, i+1, total_step, running_loss/500))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4ay9FwAkq0T"
      },
      "source": [
        "### Validate and test model results\n",
        "Validating and testing your model is an essential step in the machine learning pipeline as it provides a way to assess the performance of your model on data it has not seen before. During training, your model learns to optimize its parameters based on the training data, but it's important to check if the model can generalize to unseen data.\n",
        "\n",
        "Validation and testing allow you to estimate the model's performance on new data, giving you an idea of how well the model is likely to perform in the real world. By comparing the performance on the training set, validation set, and test set, you can check if your model is overfitting, underfitting, or generalizing well. This information can help you improve your model by adjusting its architecture or changing the hyperparameters.\n",
        "\n",
        "In summary, validating and testing your model is critical for ensuring that it performs well on new data and helps you to make informed decisions about model design and hyperparameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vewm3puu5JCi"
      },
      "source": [
        "#### Validation Phase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdb7YmXi5MrG",
        "outputId": "16e925a5-74d5-47cf-da14-900247fcb0ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN1: \n",
            "\n",
            "Validation Loss: 2.3035, Validation Accuracy: 6.65%\n"
          ]
        }
      ],
      "source": [
        "val_loss = 0\n",
        "val_accuracy = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in valid_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model1(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        val_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        val_accuracy += (predicted == targets).sum().item()\n",
        "\n",
        "val_loss /= len(valid_loader.dataset)\n",
        "val_accuracy /= len(valid_loader.dataset)\n",
        "\n",
        "print(\"CNN1: \\n\")\n",
        "print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {100 * val_accuracy:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXLfmHqF5hPz",
        "outputId": "f5e28c69-1620-48fc-a8ba-97c38786f8b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN2: \n",
            "\n",
            "Validation Loss: 2.3024, Validation Accuracy: 10.08%\n"
          ]
        }
      ],
      "source": [
        "val_loss = 0\n",
        "val_accuracy = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in valid_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model2(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        val_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        val_accuracy += (predicted == targets).sum().item()\n",
        "\n",
        "val_loss /= len(valid_loader.dataset)\n",
        "val_accuracy /= len(valid_loader.dataset)\n",
        "\n",
        "print(\"CNN2: \\n\")\n",
        "print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {100 * val_accuracy:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZzzTEYF5tdA",
        "outputId": "8b908169-a92a-47f0-dce5-b0bdbd04d360"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN3: \n",
            "\n",
            "Validation Loss: 2.3024, Validation Accuracy: 8.18%\n"
          ]
        }
      ],
      "source": [
        "val_loss = 0\n",
        "val_accuracy = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in valid_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model3(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        val_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        val_accuracy += (predicted == targets).sum().item()\n",
        "\n",
        "val_loss /= len(valid_loader.dataset)\n",
        "val_accuracy /= len(valid_loader.dataset)\n",
        "\n",
        "print(\"CNN3: \\n\")\n",
        "print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {100 * val_accuracy:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaNOlC6g5Eus"
      },
      "source": [
        "#### Test Phase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtxCkEHwZn5j",
        "outputId": "7f7e3a97-3f6c-4ccc-a0b5-0fe463240b68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the network (CNN1) on the test images: 7 %\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        outputs = model1(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network (CNN1) on the test images: %d %%' % (\n",
        "    100 * correct / total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyL9vrRu2Dip",
        "outputId": "c1b176c3-3911-4010-c6c4-5315319021f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the network (CNN2) on the test images: 9 %\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        outputs = model2(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network (CNN2) on the test images: %d %%' % (\n",
        "    100 * correct / total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJAda9Ee2HWa",
        "outputId": "58e70086-2cec-4a6c-dc8d-3689b3355008"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the network (CNN3) on the test images: 7 %\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        outputs = model3(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network (CNN3) on the test images: %d %%' % (\n",
        "    100 * correct / total))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q&A\n",
        "Which of the models had the least amount of error for validation? \n",
        "\n",
        "\n",
        "```\n",
        "While both CNN2 and CNN3 had a validation loss of 2.3024, \n",
        "CNN2 had a higher validation accuracy with 10.08%.\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "How long it took to train each model?\n",
        "\n",
        "\n",
        "```\n",
        "CNN1 took aproximately 20 minutes to train.\n",
        "CNN2 took more than an hour to train.\n",
        "CNN3 took approximately 20 minutes to train.\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "CyItazTyvJUE"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "WOiN3nQ5irvu"
      ],
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}